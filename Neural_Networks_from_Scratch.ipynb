{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_from_Scratch_New.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN-mXusnfS8R"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0ZjDoqrhpyJ"
      },
      "source": [
        "<h1> Single Neuron with 4 inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcurrlSEgNTw",
        "outputId": "77033439-f05e-4936-cd1a-f8aad2b01529"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\r\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\r\n",
        "b = 2 \r\n",
        "outputs = 0\r\n",
        "for i in range(len(inputs)):\r\n",
        "  outputs = outputs + (inputs[i]*weights[i])\r\n",
        "outputs = outputs + b\r\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5cf3V8Lh7_r"
      },
      "source": [
        "<h2> 3 Neurons with 4 inputs </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJHAK-Ixh60j",
        "outputId": "5d0be173-f8bd-4ffe-9643-e501c926af74"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\r\n",
        "weights1 = [0.2, 0.8, -0.5, 1]\r\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\r\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\r\n",
        "bias1 = 2\r\n",
        "bias2 = 3\r\n",
        "bias3 = 0.5\r\n",
        "outputs = []\r\n",
        "neurons = 3\r\n",
        "dictionary = {'weights1' : weights1,'weights2' : weights2, 'weights3' : weights3, 'bias1' : 2, 'bias2' : 3, 'bias3' : 0.5}\r\n",
        "for i in range(neurons):\r\n",
        "  weights = dictionary.get('weights'+str(i+1))\r\n",
        "  bias = dictionary.get('bias' + str(i+1))\r\n",
        "  sum = 0\r\n",
        "  for value in range(len(inputs)):\r\n",
        "    sum = sum + (inputs[value]*weights[value])\r\n",
        "      \r\n",
        "  outputs.append(sum+bias)\r\n",
        "print(outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVjzPwNuk2OB",
        "outputId": "70ede851-520e-40fc-868c-0d6f6598ae19"
      },
      "source": [
        "weights = [[0.2, 0.8, -0.5, 1],\r\n",
        " [0.5, -0.91, 0.26, -0.5],\r\n",
        " [-0.26, -0.27, 0.17, 0.87]]\r\n",
        "inputs = [1, 2, 3, 2.5]\r\n",
        "biases = [2, 3, 0.5]\r\n",
        "output = []\r\n",
        "for neuron_weights,neuron_biases in zip(weights,biases):\r\n",
        "  sum = 0\r\n",
        "  for input,weight in zip(inputs,neuron_weights):\r\n",
        "    sum += input*weight\r\n",
        "  sum += neuron_biases\r\n",
        "  output.append(sum)\r\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.8, 1.21, 2.385]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmxkvOycsCyN"
      },
      "source": [
        "<h2> Single neuron using Numpy </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_zbB095nCWz",
        "outputId": "deba1d23-6ab0-4710-d09b-1c9dd282dc79"
      },
      "source": [
        "import numpy as np\r\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\r\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\r\n",
        "bias = 2.0\r\n",
        "output = np.dot(inputs,weights) + bias\r\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUKsVoJotxtM"
      },
      "source": [
        "<h2> A Layer of Neurons with NumPy </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PC_zV2Sswm1",
        "outputId": "0eef6cb4-47b6-4026-b8e2-a195d185626d"
      },
      "source": [
        "inputs = [1.0, 2.0, 3.0, 2.5]\r\n",
        "weights = [[0.2, 0.8, -0.5, 1],\r\n",
        " [0.5, -0.91, 0.26, -0.5],\r\n",
        " [-0.26, -0.27, 0.17, 0.87]]\r\n",
        "biases = [2.0, 3.0, 0.5]\r\n",
        "\r\n",
        "output = np.dot(weights,inputs) + biases\r\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.8  , 1.21 , 2.385])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bMtpbTPt0vx"
      },
      "source": [
        "<h2> A layer of Neurons with batches of input </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5l6qhvUfG4Y",
        "outputId": "3813f2de-9796-4381-bd00-6e4602fe2a8b"
      },
      "source": [
        "inputs = [[1.0, 2.0, 3.0, 2.5],\r\n",
        " [2.0, 5.0, -1.0, 2.0],\r\n",
        " [-1.5, 2.7, 3.3, -0.8]]\r\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\r\n",
        " [0.5, -0.91, 0.26, -0.5],\r\n",
        " [-0.26, -0.27, 0.17, 0.87]]\r\n",
        "biases = [2.0, 3.0, 0.5]\r\n",
        "\r\n",
        "outputs = np.dot(inputs,np.array(weights).T) + biases\r\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.8  ,  1.21 ,  2.385],\n",
              "       [ 8.9  , -1.81 ,  0.2  ],\n",
              "       [ 1.41 ,  1.051,  0.026]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blH-Lr4yhN59"
      },
      "source": [
        "<h2> Addition of layer </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt6F1bhVhRT5",
        "outputId": "89318034-023b-47c8-f466-bd25a42a9115"
      },
      "source": [
        "inputs = [[1.0, 2.0, 3.0, 2.5],\r\n",
        " [2.0, 5.0, -1.0, 2.0],\r\n",
        " [-1.5, 2.7, 3.3, -0.8]]\r\n",
        "weights1 = [[0.2, 0.8, -0.5, 1.0],\r\n",
        " [0.5, -0.91, 0.26, -0.5],\r\n",
        " [-0.26, -0.27, 0.17, 0.87]]\r\n",
        "biases1 = [2.0, 3.0, 0.5]\r\n",
        "\r\n",
        "weights2 = [[0.1, -0.14, 0.5],\r\n",
        " [-0.5, 0.12, -0.33],\r\n",
        " [-0.44, 0.73, -0.13]]\r\n",
        "biases2 = [-1, 2, -0.5]\r\n",
        "\r\n",
        "layer1_output = np.dot(inputs,np.array(weights1).T) + biases1\r\n",
        "layer2_output = np.dot(layer1_output,np.array(weights2).T) + biases2\r\n",
        "layer2_output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.5031 , -1.04185, -2.03875],\n",
              "       [ 0.2434 , -2.7332 , -5.7633 ],\n",
              "       [-0.99314,  1.41254, -0.35655]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wcFScj2ifgs",
        "outputId": "9ab603da-054c-4ac5-bbaa-9b0b9044ebd6"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GZH9UWRuRiH"
      },
      "source": [
        "<h2> Dense layer </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rayiBHOXin0_"
      },
      "source": [
        "from nnfs.datasets import spiral_data\r\n",
        "import nnfs\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "nnfs.init()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "YWdhEFGProNd",
        "outputId": "4fc4c810-783f-45f9-d0fb-567ed9223630"
      },
      "source": [
        "X,y = spiral_data(100,3)\r\n",
        "plt.scatter(X[:,0],X[:,1],c=y,cmap='brg')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNt4B5rYu143",
        "outputId": "9d61aebf-14ff-45f4-d63f-2946d8a383a7"
      },
      "source": [
        "a = np.random.randn(2,3)\r\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.3065269 ,  1.6581306 , -0.11816405],\n",
              "       [-0.6801782 ,  0.6663831 , -0.4607198 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uru2RI9TrvwA",
        "outputId": "306beec5-7f4a-493f-f416-7840a3680820"
      },
      "source": [
        "nnfs.init()\r\n",
        "class Dense:\r\n",
        "  def __init__(self,n_inputs,n_neurons):\r\n",
        "    self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\r\n",
        "    self.biases = np.zeros((1,n_neurons))\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\r\n",
        "\r\n",
        "X,y = spiral_data(100,3)\r\n",
        "\r\n",
        "dense1 = Dense(2,3)\r\n",
        "dense1.forward(X)\r\n",
        "\r\n",
        "print(dense1.output[:5])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-5CG2ubgQY"
      },
      "source": [
        "<h2> Relu Activation </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSP-hbfubjnc"
      },
      "source": [
        "class Activation_Relu:\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output =  np.maximum(0,inputs)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XetLgeINcI84",
        "outputId": "c171c1e6-3990-4184-c126-070d11d6336c"
      },
      "source": [
        "activation = Activation_Relu()\r\n",
        "\r\n",
        "activation.forward(dense1.output)\r\n",
        "\r\n",
        "activation.output[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.00011395, 0.        ],\n",
              "       [0.        , 0.00031729, 0.        ],\n",
              "       [0.        , 0.00052666, 0.        ],\n",
              "       [0.        , 0.00071401, 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPzcnipDIYxO"
      },
      "source": [
        "<h2> Softmax activation </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS8FhvXMIb57"
      },
      "source": [
        "class SoftMax:\r\n",
        "  def forward(self,inputs):\r\n",
        "    exp = np.exp(inputs - np.max(inputs,axis=1,keepdims=True)) # subtracting values with maximum values of that particular row in order to solve problem of exploding gradient and dead neuron\r\n",
        "\r\n",
        "    probabilities = exp / np.sum(exp,axis=1,keepdims=True)\r\n",
        "    self.output = probabilities\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dscufJbw_A2U",
        "outputId": "8aa40e05-d555-4be4-ae15-a45d16f792b5"
      },
      "source": [
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,3)\r\n",
        "\r\n",
        "activation1 = Activation_Relu()\r\n",
        "\r\n",
        "dense2 = Dense(3,3)\r\n",
        "\r\n",
        "activation2 = SoftMax()\r\n",
        "\r\n",
        "dense1.forward(X)\r\n",
        "\r\n",
        "activation1.forward(dense1.output)\r\n",
        "\r\n",
        "dense2.forward(activation1.output)\r\n",
        "\r\n",
        "activation2.forward(dense2.output)\r\n",
        "\r\n",
        "activation2.output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puK_jr4xRQj0"
      },
      "source": [
        "<h1> Loss function </h1>\r\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOpHbWhSROMM"
      },
      "source": [
        "<h2> Cross entropy </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YVhdZIS-3hy",
        "outputId": "17ac7da5-0932-4e83-e2fa-0c843a6a110d"
      },
      "source": [
        "# An example output from the output layer of the neural network\r\n",
        "softmax_output = [0.7, 0.1, 0.2]\r\n",
        "# Ground truth\r\n",
        "target_output = [1, 0, 0]\r\n",
        "\r\n",
        "output = -(np.log(softmax_output[0])*target_output[0] + np.log(softmax_output[1])*target_output[1] + np.log(softmax_output[2])*target_output[2])\r\n",
        "output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35667494393873245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EENKOVCu0GJR",
        "outputId": "0eed4f7a-3961-4ed6-f480-4262055129fe"
      },
      "source": [
        "softmax_outputs = [[0.7, 0.1, 0.2],\r\n",
        " [0.1, 0.5, 0.4],\r\n",
        " [0.02, 0.9, 0.08]]\r\n",
        "class_targets = [0, 1, 1]\r\n",
        "\r\n",
        "for target_idx,output in zip(class_targets,softmax_outputs):\r\n",
        "  print(output[target_idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7\n",
            "0.5\n",
            "0.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGEY1u-V0WAr"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\r\n",
        " [0.1, 0.5, 0.4],\r\n",
        " [0.02, 0.9, 0.08]])\r\n",
        "class_targets = [0, 1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKD47i180Z-p",
        "outputId": "150c027f-9242-4b98-9f5e-577da9a3a759"
      },
      "source": [
        "softmax_outputs[[0,1,2],class_targets]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.7, 0.5, 0.9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW6xdSs61pse",
        "outputId": "d57e2945-99c7-4347-912a-60f55ece92bc"
      },
      "source": [
        "res = -np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\r\n",
        "np.mean(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38506088005216804"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONQMM0Uf23FK"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\r\n",
        " [0.1, 0.5, 0.4],\r\n",
        " [0.02, 0.9, 0.08]])\r\n",
        "class_targets = np.array([[1, 0, 0],\r\n",
        " [0, 1, 0],\r\n",
        " [0, 1, 0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kVxFdC6B-Ox"
      },
      "source": [
        "class Loss:\r\n",
        "\r\n",
        "  def calculate(self,outputs,y):\r\n",
        "\r\n",
        "    loss = self.forward(outputs,y)\r\n",
        "    \r\n",
        "    mean_loss = np.mean(loss)\r\n",
        "\r\n",
        "    return mean_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6m1CP2pCYbh"
      },
      "source": [
        "class Categorical_Crossentropy(Loss):\r\n",
        "\r\n",
        "  # class to compute categorical cross entropy loss\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "    \r\n",
        "    samples = len(y_pred)\r\n",
        "\r\n",
        "    \r\n",
        "    y_pred = np.clip(y_pred,1e-7,1 - 1e-7)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 1:\r\n",
        "      # if targets are not one hot encoded i.e targets are in the form of [target_of_sample1,target_of_sample2]\r\n",
        "      y_pred = y_pred[range(samples),y_true]\r\n",
        "\r\n",
        "    elif len(y_true.shape) == 2:\r\n",
        "\r\n",
        "      # if targets are one hot encoded i.e targets are of the shape (n_samples,n_targets) i.e [[1,0,0]]\r\n",
        "\r\n",
        "      y_pred = np.sum(y_pred * y_true,axis=1)\r\n",
        "\r\n",
        "    res = -np.log(y_pred)\r\n",
        "\r\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqQA9WfgFdQR",
        "outputId": "d6a072e1-0205-4637-b792-5587edae053f"
      },
      "source": [
        "loss = Categorical_Crossentropy()\r\n",
        "\r\n",
        "loss.calculate(softmax_outputs,class_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38506088005216804"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdT_GzbHFySZ"
      },
      "source": [
        "<h2> Combining everything till this point </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LampIEo_F2US"
      },
      "source": [
        "class Dense:\r\n",
        "\r\n",
        "  def __init__(self,n_samples,n_neurons):\r\n",
        "    self.weights = 0.01 * np.random.randn(n_samples,n_neurons)\r\n",
        "    self.biases = np.zeros((1,n_neurons))\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\r\n",
        "\r\n",
        "class ActivationRelu:\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.maximum(0,inputs)\r\n",
        "\r\n",
        "class ActivationSoftmax:\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    # inputs = (n_samples,n_classes)\r\n",
        "    exp = np.exp(inputs - np.max(inputs,axis=1,keepdims=True)) # subtracting with largets of inputs in order to solve problem of exploding gradient for example np.exp(10000) \r\n",
        "\r\n",
        "    self.output = exp / np.sum(exp,axis=1,keepdims=True) # row wise sum for each sample output shape will be output = (n_samples,n_classes)\r\n",
        "\r\n",
        "    return self.output\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Loss:\r\n",
        "\r\n",
        "  def calculate(self,outputs,y):\r\n",
        "\r\n",
        "    loss_value = self.forward(outputs,y)\r\n",
        "\r\n",
        "    mean_value = np.mean(loss_value)\r\n",
        "  \r\n",
        "    return mean_value\r\n",
        "\r\n",
        "class Categorical_Crossentropy(Loss):\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "    samples = len(y_pred) \r\n",
        "\r\n",
        "    y_pred = np.clip(y_pred,1e-7, 1 - 1e-7)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 1:\r\n",
        "      y_pred  = y_pred[range(samples),y_true]\r\n",
        "\r\n",
        "    elif len(y_true.shape) == 2:\r\n",
        "      y_pred = np.sum(y_pred*y_true,axis=1)\r\n",
        "\r\n",
        "    probabilities = -np.log(y_pred)\r\n",
        "    return probabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVxysj4LKeDE",
        "outputId": "33e5dd63-aded-4ec4-8c88-beaf602d194d"
      },
      "source": [
        "# Create dataset\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,3)\r\n",
        "\r\n",
        "dense1.forward(X)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "activation1.forward(dense1.output)\r\n",
        "\r\n",
        "dense2 = Dense(3,3)\r\n",
        "\r\n",
        "dense2.forward(activation1.output)\r\n",
        "\r\n",
        "activation2 = ActivationSoftmax()\r\n",
        "\r\n",
        "activation2.forward(dense2.output)\r\n",
        "\r\n",
        "print(activation2.output[:5])\r\n",
        "\r\n",
        "loss = Categorical_Crossentropy()\r\n",
        "\r\n",
        "loss.calculate(activation2.output,y)\r\n",
        "\r\n",
        "\r\n",
        "predictions = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "\r\n",
        "if len(y.shape) == 2:\r\n",
        "  y = np.argmax(y,axis=1)\r\n",
        "acc = np.mean(predictions==y) \r\n",
        "print(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33333412 0.3333327  0.33333313]\n",
            " [0.33333495 0.333332   0.33333302]\n",
            " [0.3333358  0.3333313  0.33333293]\n",
            " [0.33333617 0.33333114 0.3333327 ]]\n",
            "0.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDcPRxU8Nqnr"
      },
      "source": [
        "<h2> Accuracy Calculation </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ2gAgeXNtsc",
        "outputId": "f5c76b4d-8f18-4afc-9579-212437bdfdcc"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.2, 0.1],\r\n",
        " [0.5, 0.1, 0.4],\r\n",
        " [0.02, 0.9, 0.08]])\r\n",
        "# Target (ground-truth) labels for 3 samples\r\n",
        "class_targets = np.array([0, 1, 1])\r\n",
        "\r\n",
        "np.mean(np.argmax(softmax_outputs,axis=1) == class_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CefpplF1oPF2"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRWKdAxneMX6"
      },
      "source": [
        "<h2> Back propagation </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHCdC674eLrn"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "dvalues = np.array([[1., 1., 1.],\r\n",
        " [2., 2., 2.],\r\n",
        " [3., 3., 3.]])\r\n",
        "\r\n",
        "inputs = np.array([[1, 2, 3, 2.5],\r\n",
        " [2., 5., -1., 2],\r\n",
        " [-1.5, 2.7, 3.3, -0.8]])\r\n",
        "\r\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1],\r\n",
        " [0.5, -0.91, 0.26, -0.5],\r\n",
        " [-0.26, -0.27, 0.17, 0.87]]).T\r\n",
        "\r\n",
        "\r\n",
        "biases = np.array([[2, 3, 0.5]])\r\n",
        "\r\n",
        "\r\n",
        "layer_outputs = np.dot(inputs,weights) + biases\r\n",
        "relu_outputs = np.maximum(0,layer_outputs)\r\n",
        "\r\n",
        "\r\n",
        "drelu = dvalues.copy()\r\n",
        "drelu[relu_outputs <= 0] = 0\r\n",
        "\r\n",
        "dweights = np.dot(inputs.T,drelu)\r\n",
        "\r\n",
        "dinputs = np.dot(drelu,weights.T)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac8sOSk7jHKM"
      },
      "source": [
        "class Dense:\r\n",
        "\r\n",
        "  def __init__(self,n_samples,n_neurons):\r\n",
        "    self.weights = 0.01 * np.random.randn(n_samples,n_neurons)\r\n",
        "    self.biases = np.zeros((1,n_neurons))\r\n",
        "    \r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\r\n",
        "    self.inputs = inputs\r\n",
        "\r\n",
        "  def backward(self,dvalues):\r\n",
        "\r\n",
        "    self.dweights = np.dot(self.inputs.T,dvalues)\r\n",
        "    self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\r\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\r\n",
        "\r\n",
        "class ActivationRelu:\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.maximum(0,inputs)\r\n",
        "    self.inputs = inputs\r\n",
        "  \r\n",
        "  def backward(self,dvalues):\r\n",
        "\r\n",
        "    self.drelu = dvalues.copy()\r\n",
        "    self.drelu[self.inputs<=0] = 0\r\n",
        "\r\n",
        "\r\n",
        "class ActivationSoftmax:\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    \r\n",
        "    exp = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    self.output = exp / np.sum(exp,axis=1,keepdims=True) # row wise sum for each sample output shape will be output = (n_samples,n_classes)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Loss:\r\n",
        "\r\n",
        "  def calculate(self,outputs,y):\r\n",
        "\r\n",
        "    self.forward(outputs,y)\r\n",
        "\r\n",
        "    loss_value = self.probabilities\r\n",
        "\r\n",
        "    mean_value = np.mean(loss_value)\r\n",
        "  \r\n",
        "    return mean_value\r\n",
        "\r\n",
        "class Categorical_Crossentropy(Loss):\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "    samples = len(y_pred) \r\n",
        "\r\n",
        "    y_pred = np.clip(y_pred,1e-7, 1 - 1e-7)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 1:\r\n",
        "      y_pred  = y_pred[range(samples),y_true]\r\n",
        "\r\n",
        "    elif len(y_true.shape) == 2:\r\n",
        "      y_pred = np.sum(y_pred*y_true,axis=1)\r\n",
        "\r\n",
        "    self.probabilities = -np.log(y_pred)\r\n",
        "\r\n",
        "\r\n",
        "class Softmax_Activation_Crossentropy:\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.activation = ActivationSoftmax()\r\n",
        "    self.loss = Categorical_Crossentropy()\r\n",
        "  \r\n",
        "  def forward(self,inputs,y_true):\r\n",
        "\r\n",
        "    self.activation.forward(inputs)\r\n",
        "    \r\n",
        "    self.output = self.activation.output\r\n",
        "\r\n",
        "    \r\n",
        "    return self.loss.calculate(self.output,y_true)\r\n",
        "\r\n",
        "\r\n",
        "  def backward(self,dvalues,y_true):\r\n",
        "\r\n",
        "    samples = len(dvalues)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 2:\r\n",
        "      y_true = np.argmax(y_true,axis=1)\r\n",
        "\r\n",
        "    self.dinputs = dvalues.copy()\r\n",
        "    # derivative of softmax is y_i_j - y_hat_i_j\r\n",
        "    self.dinputs[range(samples),y_true] -= 1\r\n",
        "\r\n",
        "    self.dinputs = self.dinputs / samples\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xh33p3pneSb",
        "outputId": "1b1a459c-befd-433e-994c-b419f83ec373"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,3)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(3,3)\r\n",
        "\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "dense1.forward(X)\r\n",
        "\r\n",
        "print(dense1.output[:5])\r\n",
        "\r\n",
        "activation1.forward(dense1.output)\r\n",
        "\r\n",
        "dense2.forward(activation1.output)\r\n",
        "\r\n",
        "\r\n",
        "loss = activation2.forward(dense2.output,y)\r\n",
        "print(loss)\r\n",
        "\r\n",
        "prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "accuracy = np.mean(prediction == y)\r\n",
        "\r\n",
        "\r\n",
        "print(accuracy)\r\n",
        "\r\n",
        "activation2.backward(activation2.output,y)\r\n",
        "dense2.backward(activation2.dinputs)\r\n",
        "activation1.backward(dense2.dinputs)\r\n",
        "dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "activation2.dinputs[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [ 1.3171324e-05  1.5636622e-04 -3.7923790e-05]\n",
            " [-2.5519877e-04  4.5124092e-04  4.4624896e-05]\n",
            " [-3.2183601e-04  7.0533255e-04  2.9260027e-05]\n",
            " [-1.5462888e-04  8.3128718e-04 -8.3571489e-05]]\n",
            "1.098608\n",
            "0.35\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00222222,  0.00111111,  0.00111111],\n",
              "       [-0.00222222,  0.00111111,  0.00111111],\n",
              "       [-0.00222222,  0.00111111,  0.00111111],\n",
              "       [-0.00222222,  0.00111111,  0.00111111],\n",
              "       [-0.00222222,  0.00111112,  0.0011111 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5pzs-CYYbU5"
      },
      "source": [
        "<h2> Optimizers </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRm0KpJBnY8h"
      },
      "source": [
        "# SGD optimizers\r\n",
        "class SGD:\r\n",
        "\r\n",
        "  def __init__(self,learning_rate=1,rate_decay=0.):\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.rate_decay = rate_decay\r\n",
        "    self.iterations = 0\r\n",
        "    self.current_learning_rate = learning_rate\r\n",
        "\r\n",
        "  \r\n",
        "\r\n",
        "  def update_learning_rate(self):\r\n",
        "\r\n",
        "    if self.rate_decay:\r\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.rate_decay * self.iterations))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def update_params(self,layer):\r\n",
        "    layer.weights -= self.current_learning_rate*layer.dweights\r\n",
        "    layer.biases -= self.current_learning_rate*layer.dbiases\r\n",
        "  \r\n",
        "  def update_iterations(self):\r\n",
        "    self.iterations += 1\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9jcV8PWboC"
      },
      "source": [
        "class SGD_Momentum:\r\n",
        "  \r\n",
        "  def __init__(self,learning_rate=1,rate_decay=0.,momentum = 0.):\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.rate_decay = rate_decay\r\n",
        "    self.iterations = 0\r\n",
        "    self.current_learning_rate = learning_rate\r\n",
        "    self.momentum = momentum\r\n",
        "\r\n",
        "  def update_learning_rate(self):\r\n",
        "\r\n",
        "    if self.rate_decay:\r\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.rate_decay * self.iterations))\r\n",
        "\r\n",
        "  def update_params(self,layer):\r\n",
        "    if self.momentum:\r\n",
        "      if not hasattr(layer,'weight_momentum'):\r\n",
        "        layer.weight_momentum = np.zeros_like(layer.weights)\r\n",
        "\r\n",
        "        layer.bias_momentum = np.zeros_like(layer.biases)\r\n",
        "\r\n",
        "      weight_updates = self.momentum * layer.weight_momentum - self.current_learning_rate * layer.dweights\r\n",
        "      biases_updates = self.momentum * layer.bias_momentum - self.current_learning_rate * layer.dbiases\r\n",
        "      layer.weight_momentum = weight_updates\r\n",
        "      layer.bias_momentum - biases_updates\r\n",
        "    else:\r\n",
        "      weight_updates -= self.current_learning_rate * layer.dweights\r\n",
        "      biases_updates -= self.current_learning_rate * layer.dbiases\r\n",
        "\r\n",
        "    layer.weights += weight_updates\r\n",
        "    layer.biases += biases_updates\r\n",
        "\r\n",
        "  def update_iterations(self):\r\n",
        "    self.iterations += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHSbTqbSSjex",
        "outputId": "f1c51f9c-ca3a-4fa5-cd33-17680c3e083f"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,3)\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "optimizers = SGD(rate_decay=1e-3)\r\n",
        "for epoch in range(20001):\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  loss = activation2.forward(dense2.output,y)\r\n",
        "\r\n",
        "  prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "  acc = np.mean(prediction==y)\r\n",
        "\r\n",
        "  if not epoch % 100 :\r\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epoch,loss,acc,optimizers.current_learning_rate))\r\n",
        "\r\n",
        "  activation2.backward(activation2.output,y)\r\n",
        "\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizers.update_learning_rate()\r\n",
        "  optimizers.update_params(dense1)\r\n",
        "  optimizers.update_params(dense2)\r\n",
        "  optimizers.update_iterations()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.098588228225708 and acc is 0.3433333333333333, learning_Rate is 1\n",
            "epoch is 100,loss is 1.0895360708236694 and acc is 0.4066666666666667, learning_Rate is 0.9099181073703367\n",
            "epoch is 200,loss is 1.0785685777664185 and acc is 0.4066666666666667, learning_Rate is 0.8340283569641367\n",
            "epoch is 300,loss is 1.076603651046753 and acc is 0.41, learning_Rate is 0.7698229407236336\n",
            "epoch is 400,loss is 1.0759880542755127 and acc is 0.4066666666666667, learning_Rate is 0.7147962830593281\n",
            "epoch is 500,loss is 1.0752710103988647 and acc is 0.4033333333333333, learning_Rate is 0.66711140760507\n",
            "epoch is 600,loss is 1.0739953517913818 and acc is 0.41, learning_Rate is 0.6253908692933083\n",
            "epoch is 700,loss is 1.072247862815857 and acc is 0.4166666666666667, learning_Rate is 0.5885815185403178\n",
            "epoch is 800,loss is 1.0700596570968628 and acc is 0.42, learning_Rate is 0.5558643690939411\n",
            "epoch is 900,loss is 1.0678051710128784 and acc is 0.4266666666666667, learning_Rate is 0.526592943654555\n",
            "epoch is 1000,loss is 1.0653979778289795 and acc is 0.4266666666666667, learning_Rate is 0.5002501250625312\n",
            "epoch is 1100,loss is 1.0626633167266846 and acc is 0.43, learning_Rate is 0.4764173415912339\n",
            "epoch is 1200,loss is 1.0593966245651245 and acc is 0.43666666666666665, learning_Rate is 0.45475216007276037\n",
            "epoch is 1300,loss is 1.0553241968154907 and acc is 0.44, learning_Rate is 0.43497172683775553\n",
            "epoch is 1400,loss is 1.0503056049346924 and acc is 0.45, learning_Rate is 0.4168403501458941\n",
            "epoch is 1500,loss is 1.0446463823318481 and acc is 0.47, learning_Rate is 0.4001600640256102\n",
            "epoch is 1600,loss is 1.0386847257614136 and acc is 0.4766666666666667, learning_Rate is 0.3847633705271258\n",
            "epoch is 1700,loss is 1.0328779220581055 and acc is 0.47333333333333333, learning_Rate is 0.3705075954057058\n",
            "epoch is 1800,loss is 1.0275697708129883 and acc is 0.46, learning_Rate is 0.35727045373347627\n",
            "epoch is 1900,loss is 1.022765040397644 and acc is 0.4633333333333333, learning_Rate is 0.3449465332873405\n",
            "epoch is 2000,loss is 1.0175789594650269 and acc is 0.4766666666666667, learning_Rate is 0.33344448149383127\n",
            "epoch is 2100,loss is 1.0131158828735352 and acc is 0.48, learning_Rate is 0.32268473701193934\n",
            "epoch is 2200,loss is 1.0088244676589966 and acc is 0.49666666666666665, learning_Rate is 0.31259768677711786\n",
            "epoch is 2300,loss is 1.0035316944122314 and acc is 0.5033333333333333, learning_Rate is 0.3031221582297666\n",
            "epoch is 2400,loss is 0.9981781840324402 and acc is 0.51, learning_Rate is 0.29420417769932333\n",
            "epoch is 2500,loss is 0.9933339357376099 and acc is 0.51, learning_Rate is 0.2857959416976279\n",
            "epoch is 2600,loss is 0.9888244867324829 and acc is 0.49, learning_Rate is 0.2778549597110308\n",
            "epoch is 2700,loss is 0.9842710494995117 and acc is 0.49333333333333335, learning_Rate is 0.2703433360367667\n",
            "epoch is 2800,loss is 0.9797359108924866 and acc is 0.51, learning_Rate is 0.26322716504343247\n",
            "epoch is 2900,loss is 0.974560558795929 and acc is 0.5233333333333333, learning_Rate is 0.25647601949217746\n",
            "epoch is 3000,loss is 0.9695389866828918 and acc is 0.5333333333333333, learning_Rate is 0.25006251562890724\n",
            "epoch is 3100,loss is 0.9733800292015076 and acc is 0.54, learning_Rate is 0.2439619419370578\n",
            "epoch is 3200,loss is 0.9767566919326782 and acc is 0.5333333333333333, learning_Rate is 0.23815194093831865\n",
            "epoch is 3300,loss is 0.9765751361846924 and acc is 0.5233333333333333, learning_Rate is 0.23261223540358225\n",
            "epoch is 3400,loss is 0.9731679558753967 and acc is 0.5233333333333333, learning_Rate is 0.22732439190725165\n",
            "epoch is 3500,loss is 0.9697532057762146 and acc is 0.5233333333333333, learning_Rate is 0.22227161591464767\n",
            "epoch is 3600,loss is 0.9659727215766907 and acc is 0.5333333333333333, learning_Rate is 0.21743857360295715\n",
            "epoch is 3700,loss is 0.9647690653800964 and acc is 0.5333333333333333, learning_Rate is 0.21281123643328367\n",
            "epoch is 3800,loss is 0.9613232612609863 and acc is 0.5366666666666666, learning_Rate is 0.20837674515524068\n",
            "epoch is 3900,loss is 0.9555125832557678 and acc is 0.5366666666666666, learning_Rate is 0.20412329046744235\n",
            "epoch is 4000,loss is 0.952280580997467 and acc is 0.5466666666666666, learning_Rate is 0.2000400080016003\n",
            "epoch is 4100,loss is 0.9492214918136597 and acc is 0.5533333333333333, learning_Rate is 0.19611688566385566\n",
            "epoch is 4200,loss is 0.9436835646629333 and acc is 0.5433333333333333, learning_Rate is 0.19234468166955185\n",
            "epoch is 4300,loss is 0.9400728344917297 and acc is 0.55, learning_Rate is 0.18871485185884126\n",
            "epoch is 4400,loss is 0.9383693337440491 and acc is 0.5566666666666666, learning_Rate is 0.18521948508983144\n",
            "epoch is 4500,loss is 0.9341198801994324 and acc is 0.56, learning_Rate is 0.18185124568103292\n",
            "epoch is 4600,loss is 0.9299283027648926 and acc is 0.5566666666666666, learning_Rate is 0.1786033220217896\n",
            "epoch is 4700,loss is 0.924778938293457 and acc is 0.5666666666666667, learning_Rate is 0.1754693805930865\n",
            "epoch is 4800,loss is 0.9223282933235168 and acc is 0.5666666666666667, learning_Rate is 0.17244352474564578\n",
            "epoch is 4900,loss is 0.917041540145874 and acc is 0.5733333333333334, learning_Rate is 0.16952025767079165\n",
            "epoch is 5000,loss is 0.9130090475082397 and acc is 0.5833333333333334, learning_Rate is 0.16669444907484582\n",
            "epoch is 5100,loss is 0.9091841578483582 and acc is 0.5833333333333334, learning_Rate is 0.16396130513198884\n",
            "epoch is 5200,loss is 0.9053424000740051 and acc is 0.5866666666666667, learning_Rate is 0.16131634134537828\n",
            "epoch is 5300,loss is 0.9012807011604309 and acc is 0.59, learning_Rate is 0.15875535799333226\n",
            "epoch is 5400,loss is 0.8966411352157593 and acc is 0.5966666666666667, learning_Rate is 0.1562744178777934\n",
            "epoch is 5500,loss is 0.8910709619522095 and acc is 0.6, learning_Rate is 0.15386982612709646\n",
            "epoch is 5600,loss is 0.8876240849494934 and acc is 0.6066666666666667, learning_Rate is 0.15153811183512653\n",
            "epoch is 5700,loss is 0.8832550048828125 and acc is 0.6066666666666667, learning_Rate is 0.14927601134497687\n",
            "epoch is 5800,loss is 0.8807106614112854 and acc is 0.61, learning_Rate is 0.14708045300779526\n",
            "epoch is 5900,loss is 0.8766255974769592 and acc is 0.6, learning_Rate is 0.14494854326714016\n",
            "epoch is 6000,loss is 0.874765932559967 and acc is 0.6, learning_Rate is 0.1428775539362766\n",
            "epoch is 6100,loss is 0.8697660565376282 and acc is 0.5933333333333334, learning_Rate is 0.1408649105507818\n",
            "epoch is 6200,loss is 0.8649015426635742 and acc is 0.6066666666666667, learning_Rate is 0.13890818169190167\n",
            "epoch is 6300,loss is 0.8587957620620728 and acc is 0.6233333333333333, learning_Rate is 0.13700506918755992\n",
            "epoch is 6400,loss is 0.8546003103256226 and acc is 0.6266666666666667, learning_Rate is 0.13515339910798757\n",
            "epoch is 6500,loss is 0.8526644110679626 and acc is 0.6333333333333333, learning_Rate is 0.13335111348179757\n",
            "epoch is 6600,loss is 0.851172685623169 and acc is 0.64, learning_Rate is 0.13159626266614027\n",
            "epoch is 6700,loss is 0.8491889238357544 and acc is 0.6433333333333333, learning_Rate is 0.12988699831146902\n",
            "epoch is 6800,loss is 0.8465111255645752 and acc is 0.6433333333333333, learning_Rate is 0.12822156686754713\n",
            "epoch is 6900,loss is 0.843130886554718 and acc is 0.64, learning_Rate is 0.126598303582732\n",
            "epoch is 7000,loss is 0.840382993221283 and acc is 0.6433333333333333, learning_Rate is 0.12501562695336915\n",
            "epoch is 7100,loss is 0.8384796380996704 and acc is 0.64, learning_Rate is 0.12347203358439313\n",
            "epoch is 7200,loss is 0.8357019424438477 and acc is 0.6366666666666667, learning_Rate is 0.12196609342602757\n",
            "epoch is 7300,loss is 0.833544135093689 and acc is 0.6333333333333333, learning_Rate is 0.12049644535486204\n",
            "epoch is 7400,loss is 0.8307328820228577 and acc is 0.63, learning_Rate is 0.11906179307060363\n",
            "epoch is 7500,loss is 0.8286810517311096 and acc is 0.6266666666666667, learning_Rate is 0.11766090128250381\n",
            "epoch is 7600,loss is 0.8268333077430725 and acc is 0.63, learning_Rate is 0.11629259216187929\n",
            "epoch is 7700,loss is 0.8250133395195007 and acc is 0.6266666666666667, learning_Rate is 0.11495574203931487\n",
            "epoch is 7800,loss is 0.8232864141464233 and acc is 0.6233333333333333, learning_Rate is 0.11364927832708263\n",
            "epoch is 7900,loss is 0.8217068314552307 and acc is 0.6233333333333333, learning_Rate is 0.11237217664906168\n",
            "epoch is 8000,loss is 0.82015061378479 and acc is 0.63, learning_Rate is 0.11112345816201799\n",
            "epoch is 8100,loss is 0.8187209963798523 and acc is 0.63, learning_Rate is 0.10990218705352237\n",
            "epoch is 8200,loss is 0.8174715638160706 and acc is 0.63, learning_Rate is 0.10870746820306555\n",
            "epoch is 8300,loss is 0.8159933090209961 and acc is 0.6333333333333333, learning_Rate is 0.1075384449940854\n",
            "epoch is 8400,loss is 0.8146291971206665 and acc is 0.63, learning_Rate is 0.10639429726566654\n",
            "epoch is 8500,loss is 0.8134865164756775 and acc is 0.6333333333333333, learning_Rate is 0.10527423939362038\n",
            "epoch is 8600,loss is 0.8122504949569702 and acc is 0.6333333333333333, learning_Rate is 0.10417751849150952\n",
            "epoch is 8700,loss is 0.8108900785446167 and acc is 0.63, learning_Rate is 0.10310341272296113\n",
            "epoch is 8800,loss is 0.8096704483032227 and acc is 0.6266666666666667, learning_Rate is 0.1020512297173181\n",
            "epoch is 8900,loss is 0.8084938526153564 and acc is 0.63, learning_Rate is 0.10102030508132134\n",
            "epoch is 9000,loss is 0.8073753118515015 and acc is 0.63, learning_Rate is 0.1000100010001\n",
            "epoch is 9100,loss is 0.806273341178894 and acc is 0.6333333333333333, learning_Rate is 0.09901970492127933\n",
            "epoch is 9200,loss is 0.8051666021347046 and acc is 0.6333333333333333, learning_Rate is 0.09804882831650162\n",
            "epoch is 9300,loss is 0.8040834665298462 and acc is 0.6333333333333333, learning_Rate is 0.09709680551509856\n",
            "epoch is 9400,loss is 0.8030077219009399 and acc is 0.6333333333333333, learning_Rate is 0.09616309260505818\n",
            "epoch is 9500,loss is 0.8019872307777405 and acc is 0.6333333333333333, learning_Rate is 0.09524716639679968\n",
            "epoch is 9600,loss is 0.8009631037712097 and acc is 0.6333333333333333, learning_Rate is 0.09434852344560807\n",
            "epoch is 9700,loss is 0.7999446988105774 and acc is 0.6333333333333333, learning_Rate is 0.09346667912889055\n",
            "epoch is 9800,loss is 0.798921525478363 and acc is 0.6333333333333333, learning_Rate is 0.09260116677470137\n",
            "epoch is 9900,loss is 0.7979706525802612 and acc is 0.63, learning_Rate is 0.09175153683824203\n",
            "epoch is 10000,loss is 0.7970104217529297 and acc is 0.63, learning_Rate is 0.09091735612328393\n",
            "epoch is 10100,loss is 0.7960894703865051 and acc is 0.63, learning_Rate is 0.09009820704567979\n",
            "epoch is 10200,loss is 0.7951582074165344 and acc is 0.63, learning_Rate is 0.0892936869363336\n",
            "epoch is 10300,loss is 0.7942520380020142 and acc is 0.63, learning_Rate is 0.08850340738118417\n",
            "epoch is 10400,loss is 0.7933285236358643 and acc is 0.6333333333333333, learning_Rate is 0.08772699359592946\n",
            "epoch is 10500,loss is 0.7924448847770691 and acc is 0.6333333333333333, learning_Rate is 0.08696408383337681\n",
            "epoch is 10600,loss is 0.791607677936554 and acc is 0.6333333333333333, learning_Rate is 0.08621432882145012\n",
            "epoch is 10700,loss is 0.790747344493866 and acc is 0.6333333333333333, learning_Rate is 0.08547739123001966\n",
            "epoch is 10800,loss is 0.7899372577667236 and acc is 0.6333333333333333, learning_Rate is 0.08475294516484448\n",
            "epoch is 10900,loss is 0.7891244292259216 and acc is 0.6333333333333333, learning_Rate is 0.08404067568703252\n",
            "epoch is 11000,loss is 0.7883021831512451 and acc is 0.63, learning_Rate is 0.08334027835652971\n",
            "epoch is 11100,loss is 0.787493109703064 and acc is 0.6366666666666667, learning_Rate is 0.08265145879824778\n",
            "epoch is 11200,loss is 0.7867214679718018 and acc is 0.6366666666666667, learning_Rate is 0.08197393228953193\n",
            "epoch is 11300,loss is 0.7858784198760986 and acc is 0.6333333333333333, learning_Rate is 0.08130742336775348\n",
            "epoch is 11400,loss is 0.7849684357643127 and acc is 0.6333333333333333, learning_Rate is 0.08065166545689167\n",
            "epoch is 11500,loss is 0.7841746211051941 and acc is 0.6333333333333333, learning_Rate is 0.08000640051204096\n",
            "epoch is 11600,loss is 0.7833428978919983 and acc is 0.6333333333333333, learning_Rate is 0.07937137868084769\n",
            "epoch is 11700,loss is 0.7825475931167603 and acc is 0.6366666666666667, learning_Rate is 0.07874635798094339\n",
            "epoch is 11800,loss is 0.7818031907081604 and acc is 0.6366666666666667, learning_Rate is 0.07813110399249942\n",
            "epoch is 11900,loss is 0.7810907959938049 and acc is 0.6366666666666667, learning_Rate is 0.07752538956508256\n",
            "epoch is 12000,loss is 0.7803748846054077 and acc is 0.6366666666666667, learning_Rate is 0.07692899453804139\n",
            "epoch is 12100,loss is 0.7796669602394104 and acc is 0.64, learning_Rate is 0.07634170547370028\n",
            "epoch is 12200,loss is 0.7789597511291504 and acc is 0.64, learning_Rate is 0.07576331540268202\n",
            "epoch is 12300,loss is 0.778277575969696 and acc is 0.6433333333333333, learning_Rate is 0.07519362358072036\n",
            "epoch is 12400,loss is 0.777584969997406 and acc is 0.6433333333333333, learning_Rate is 0.07463243525636241\n",
            "epoch is 12500,loss is 0.7769176363945007 and acc is 0.6466666666666666, learning_Rate is 0.07407956144899622\n",
            "epoch is 12600,loss is 0.7762494683265686 and acc is 0.6466666666666666, learning_Rate is 0.07353481873667181\n",
            "epoch is 12700,loss is 0.77558833360672 and acc is 0.6466666666666666, learning_Rate is 0.07299802905321556\n",
            "epoch is 12800,loss is 0.774944543838501 and acc is 0.65, learning_Rate is 0.07246901949416625\n",
            "epoch is 12900,loss is 0.7742933630943298 and acc is 0.6466666666666666, learning_Rate is 0.07194762213108856\n",
            "epoch is 13000,loss is 0.7736454010009766 and acc is 0.65, learning_Rate is 0.07143367383384527\n",
            "epoch is 13100,loss is 0.772998034954071 and acc is 0.65, learning_Rate is 0.07092701610043266\n",
            "epoch is 13200,loss is 0.7723493576049805 and acc is 0.6533333333333333, learning_Rate is 0.07042749489400663\n",
            "epoch is 13300,loss is 0.7717090845108032 and acc is 0.6533333333333333, learning_Rate is 0.06993496048674733\n",
            "epoch is 13400,loss is 0.7710864543914795 and acc is 0.6533333333333333, learning_Rate is 0.06944926731022988\n",
            "epoch is 13500,loss is 0.7704638242721558 and acc is 0.6566666666666666, learning_Rate is 0.06897027381198703\n",
            "epoch is 13600,loss is 0.7698478102684021 and acc is 0.6533333333333333, learning_Rate is 0.06849784231796699\n",
            "epoch is 13700,loss is 0.7692424654960632 and acc is 0.6533333333333333, learning_Rate is 0.06803183890060549\n",
            "epoch is 13800,loss is 0.7686474323272705 and acc is 0.6533333333333333, learning_Rate is 0.06757213325224677\n",
            "epoch is 13900,loss is 0.7680613398551941 and acc is 0.6533333333333333, learning_Rate is 0.06711859856366198\n",
            "epoch is 14000,loss is 0.7674793601036072 and acc is 0.6533333333333333, learning_Rate is 0.06667111140742715\n",
            "epoch is 14100,loss is 0.7668951153755188 and acc is 0.6533333333333333, learning_Rate is 0.0662295516259355\n",
            "epoch is 14200,loss is 0.7663232684135437 and acc is 0.66, learning_Rate is 0.06579380222383052\n",
            "epoch is 14300,loss is 0.7657434344291687 and acc is 0.66, learning_Rate is 0.06536374926465782\n",
            "epoch is 14400,loss is 0.765180230140686 and acc is 0.66, learning_Rate is 0.06493928177154361\n",
            "epoch is 14500,loss is 0.7646099925041199 and acc is 0.66, learning_Rate is 0.06452029163171817\n",
            "epoch is 14600,loss is 0.763384997844696 and acc is 0.6633333333333333, learning_Rate is 0.06410667350471184\n",
            "epoch is 14700,loss is 0.7625555396080017 and acc is 0.6633333333333333, learning_Rate is 0.0636983247340595\n",
            "epoch is 14800,loss is 0.7618044018745422 and acc is 0.6666666666666666, learning_Rate is 0.06329514526235838\n",
            "epoch is 14900,loss is 0.7611276507377625 and acc is 0.6666666666666666, learning_Rate is 0.06289703754953141\n",
            "epoch is 15000,loss is 0.7604572176933289 and acc is 0.6666666666666666, learning_Rate is 0.06250390649415588\n",
            "epoch is 15100,loss is 0.7597751617431641 and acc is 0.6666666666666666, learning_Rate is 0.06211565935772408\n",
            "epoch is 15200,loss is 0.7591440677642822 and acc is 0.6666666666666666, learning_Rate is 0.061732205691709376\n",
            "epoch is 15300,loss is 0.7585458159446716 and acc is 0.67, learning_Rate is 0.061353457267317016\n",
            "epoch is 15400,loss is 0.7579687237739563 and acc is 0.67, learning_Rate is 0.06097932800780535\n",
            "epoch is 15500,loss is 0.7574244737625122 and acc is 0.6666666666666666, learning_Rate is 0.060609733923268065\n",
            "epoch is 15600,loss is 0.7568873763084412 and acc is 0.67, learning_Rate is 0.060244593047773964\n",
            "epoch is 15700,loss is 0.7563351392745972 and acc is 0.67, learning_Rate is 0.0598838253787652\n",
            "epoch is 15800,loss is 0.7557998299598694 and acc is 0.67, learning_Rate is 0.059527352818620156\n",
            "epoch is 15900,loss is 0.7552573084831238 and acc is 0.67, learning_Rate is 0.05917509911829102\n",
            "epoch is 16000,loss is 0.7547332644462585 and acc is 0.67, learning_Rate is 0.058826989822930754\n",
            "epoch is 16100,loss is 0.7541970014572144 and acc is 0.6733333333333333, learning_Rate is 0.058482952219428036\n",
            "epoch is 16200,loss is 0.7536634206771851 and acc is 0.6733333333333333, learning_Rate is 0.05814291528577242\n",
            "epoch is 16300,loss is 0.7531376481056213 and acc is 0.6733333333333333, learning_Rate is 0.05780680964217585\n",
            "epoch is 16400,loss is 0.752568244934082 and acc is 0.6733333333333333, learning_Rate is 0.05747456750387953\n",
            "epoch is 16500,loss is 0.751996636390686 and acc is 0.67, learning_Rate is 0.05714612263557918\n",
            "epoch is 16600,loss is 0.7514414191246033 and acc is 0.6733333333333333, learning_Rate is 0.05682141030740383\n",
            "epoch is 16700,loss is 0.750869870185852 and acc is 0.67, learning_Rate is 0.056500367252387135\n",
            "epoch is 16800,loss is 0.7503198981285095 and acc is 0.6733333333333333, learning_Rate is 0.05618293162537221\n",
            "epoch is 16900,loss is 0.7497773170471191 and acc is 0.6733333333333333, learning_Rate is 0.055869042963294036\n",
            "epoch is 17000,loss is 0.7492212057113647 and acc is 0.6733333333333333, learning_Rate is 0.055558642146785936\n",
            "epoch is 17100,loss is 0.7486835718154907 and acc is 0.6733333333333333, learning_Rate is 0.05525167136305873\n",
            "epoch is 17200,loss is 0.7481580376625061 and acc is 0.6733333333333333, learning_Rate is 0.05494807407000384\n",
            "epoch is 17300,loss is 0.7476102113723755 and acc is 0.6733333333333333, learning_Rate is 0.0546477949614733\n",
            "epoch is 17400,loss is 0.7470812201499939 and acc is 0.6733333333333333, learning_Rate is 0.05435077993369205\n",
            "epoch is 17500,loss is 0.7465240359306335 and acc is 0.6766666666666666, learning_Rate is 0.05405697605275961\n",
            "epoch is 17600,loss is 0.7459583282470703 and acc is 0.6766666666666666, learning_Rate is 0.05376633152320017\n",
            "epoch is 17700,loss is 0.7454013824462891 and acc is 0.6766666666666666, learning_Rate is 0.05347879565752179\n",
            "epoch is 17800,loss is 0.7448453307151794 and acc is 0.6766666666666666, learning_Rate is 0.05319431884674717\n",
            "epoch is 17900,loss is 0.7442996501922607 and acc is 0.6766666666666666, learning_Rate is 0.05291285253187999\n",
            "epoch is 18000,loss is 0.7437447309494019 and acc is 0.6766666666666666, learning_Rate is 0.05263434917627244\n",
            "epoch is 18100,loss is 0.7432130575180054 and acc is 0.6766666666666666, learning_Rate is 0.05235876223886067\n",
            "epoch is 18200,loss is 0.7426977753639221 and acc is 0.6766666666666666, learning_Rate is 0.052086046148236885\n",
            "epoch is 18300,loss is 0.7421917915344238 and acc is 0.6766666666666666, learning_Rate is 0.05181615627752734\n",
            "epoch is 18400,loss is 0.7416941523551941 and acc is 0.6766666666666666, learning_Rate is 0.05154904892004742\n",
            "epoch is 18500,loss is 0.7412059307098389 and acc is 0.6766666666666666, learning_Rate is 0.051284681265705935\n",
            "epoch is 18600,loss is 0.7407248616218567 and acc is 0.6766666666666666, learning_Rate is 0.05102301137813154\n",
            "epoch is 18700,loss is 0.740232527256012 and acc is 0.6766666666666666, learning_Rate is 0.050763998172496064\n",
            "epoch is 18800,loss is 0.7397572994232178 and acc is 0.68, learning_Rate is 0.0505076013940098\n",
            "epoch is 18900,loss is 0.7392784953117371 and acc is 0.68, learning_Rate is 0.05025378159706518\n",
            "epoch is 19000,loss is 0.7388120889663696 and acc is 0.68, learning_Rate is 0.050002500125006254\n",
            "epoch is 19100,loss is 0.7383499145507812 and acc is 0.68, learning_Rate is 0.04975371909050202\n",
            "epoch is 19200,loss is 0.7378758788108826 and acc is 0.68, learning_Rate is 0.04950740135650279\n",
            "epoch is 19300,loss is 0.7374096512794495 and acc is 0.68, learning_Rate is 0.0492635105177595\n",
            "epoch is 19400,loss is 0.7369496822357178 and acc is 0.68, learning_Rate is 0.049022010882886415\n",
            "epoch is 19500,loss is 0.7364941239356995 and acc is 0.68, learning_Rate is 0.048782867456949125\n",
            "epoch is 19600,loss is 0.7360366582870483 and acc is 0.68, learning_Rate is 0.048546045924559446\n",
            "epoch is 19700,loss is 0.7355840802192688 and acc is 0.68, learning_Rate is 0.04831151263346055\n",
            "epoch is 19800,loss is 0.735134482383728 and acc is 0.6833333333333333, learning_Rate is 0.04807923457858551\n",
            "epoch is 19900,loss is 0.7346931099891663 and acc is 0.6833333333333333, learning_Rate is 0.047849179386573515\n",
            "epoch is 20000,loss is 0.7342530488967896 and acc is 0.6833333333333333, learning_Rate is 0.047621315300728606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBvwbyHD5UTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230c41e3-c773-431a-dda9-9f8dd1a04c35"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,3)\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "optimizers = SGD_Momentum(rate_decay=1e-3,momentum=0.5)\r\n",
        "for epoch in range(20001):\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  loss = activation2.forward(dense2.output,y)\r\n",
        "\r\n",
        "  prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "  acc = np.mean(prediction==y)\r\n",
        "\r\n",
        "  if not epoch % 100 :\r\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epoch,loss,acc,optimizers.current_learning_rate))\r\n",
        "\r\n",
        "  activation2.backward(activation2.output,y)\r\n",
        "\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizers.update_learning_rate()\r\n",
        "  optimizers.update_params(dense1)\r\n",
        "  optimizers.update_params(dense2)\r\n",
        "  optimizers.update_iterations()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.0985819101333618 and acc is 0.36666666666666664, learning_Rate is 1\n",
            "epoch is 100,loss is 1.0722851753234863 and acc is 0.43666666666666665, learning_Rate is 0.9099181073703367\n",
            "epoch is 200,loss is 1.0700246095657349 and acc is 0.41333333333333333, learning_Rate is 0.8340283569641367\n",
            "epoch is 300,loss is 1.0694432258605957 and acc is 0.41, learning_Rate is 0.7698229407236336\n",
            "epoch is 400,loss is 1.0681352615356445 and acc is 0.41, learning_Rate is 0.7147962830593281\n",
            "epoch is 500,loss is 1.0644092559814453 and acc is 0.41, learning_Rate is 0.66711140760507\n",
            "epoch is 600,loss is 1.0551961660385132 and acc is 0.4066666666666667, learning_Rate is 0.6253908692933083\n",
            "epoch is 700,loss is 1.0403854846954346 and acc is 0.4066666666666667, learning_Rate is 0.5885815185403178\n",
            "epoch is 800,loss is 1.0252550840377808 and acc is 0.42, learning_Rate is 0.5558643690939411\n",
            "epoch is 900,loss is 1.008949637413025 and acc is 0.43666666666666665, learning_Rate is 0.526592943654555\n",
            "epoch is 1000,loss is 0.9879917502403259 and acc is 0.4766666666666667, learning_Rate is 0.5002501250625312\n",
            "epoch is 1100,loss is 0.9652838110923767 and acc is 0.47333333333333333, learning_Rate is 0.4764173415912339\n",
            "epoch is 1200,loss is 0.9444712400436401 and acc is 0.5066666666666667, learning_Rate is 0.45475216007276037\n",
            "epoch is 1300,loss is 0.9267010688781738 and acc is 0.52, learning_Rate is 0.43497172683775553\n",
            "epoch is 1400,loss is 0.9113975167274475 and acc is 0.5366666666666666, learning_Rate is 0.4168403501458941\n",
            "epoch is 1500,loss is 0.8955409526824951 and acc is 0.55, learning_Rate is 0.4001600640256102\n",
            "epoch is 1600,loss is 0.8786675930023193 and acc is 0.5766666666666667, learning_Rate is 0.3847633705271258\n",
            "epoch is 1700,loss is 0.8681803345680237 and acc is 0.56, learning_Rate is 0.3705075954057058\n",
            "epoch is 1800,loss is 0.8525087833404541 and acc is 0.6066666666666667, learning_Rate is 0.35727045373347627\n",
            "epoch is 1900,loss is 0.8388826251029968 and acc is 0.61, learning_Rate is 0.3449465332873405\n",
            "epoch is 2000,loss is 0.823672354221344 and acc is 0.62, learning_Rate is 0.33344448149383127\n",
            "epoch is 2100,loss is 0.8094397187232971 and acc is 0.63, learning_Rate is 0.32268473701193934\n",
            "epoch is 2200,loss is 0.7978268265724182 and acc is 0.6433333333333333, learning_Rate is 0.31259768677711786\n",
            "epoch is 2300,loss is 0.784013032913208 and acc is 0.64, learning_Rate is 0.3031221582297666\n",
            "epoch is 2400,loss is 0.7689192891120911 and acc is 0.65, learning_Rate is 0.29420417769932333\n",
            "epoch is 2500,loss is 0.7525371313095093 and acc is 0.6633333333333333, learning_Rate is 0.2857959416976279\n",
            "epoch is 2600,loss is 0.7413740158081055 and acc is 0.67, learning_Rate is 0.2778549597110308\n",
            "epoch is 2700,loss is 0.727033793926239 and acc is 0.6766666666666666, learning_Rate is 0.2703433360367667\n",
            "epoch is 2800,loss is 0.7145024538040161 and acc is 0.6866666666666666, learning_Rate is 0.26322716504343247\n",
            "epoch is 2900,loss is 0.7032684087753296 and acc is 0.6966666666666667, learning_Rate is 0.25647601949217746\n",
            "epoch is 3000,loss is 0.694071352481842 and acc is 0.7066666666666667, learning_Rate is 0.25006251562890724\n",
            "epoch is 3100,loss is 0.6844810843467712 and acc is 0.71, learning_Rate is 0.2439619419370578\n",
            "epoch is 3200,loss is 0.6750804781913757 and acc is 0.71, learning_Rate is 0.23815194093831865\n",
            "epoch is 3300,loss is 0.6660792827606201 and acc is 0.7233333333333334, learning_Rate is 0.23261223540358225\n",
            "epoch is 3400,loss is 0.6568670868873596 and acc is 0.7333333333333333, learning_Rate is 0.22732439190725165\n",
            "epoch is 3500,loss is 0.6492347717285156 and acc is 0.7333333333333333, learning_Rate is 0.22227161591464767\n",
            "epoch is 3600,loss is 0.6407888531684875 and acc is 0.74, learning_Rate is 0.21743857360295715\n",
            "epoch is 3700,loss is 0.6331870555877686 and acc is 0.7433333333333333, learning_Rate is 0.21281123643328367\n",
            "epoch is 3800,loss is 0.6244915723800659 and acc is 0.7433333333333333, learning_Rate is 0.20837674515524068\n",
            "epoch is 3900,loss is 0.6193568706512451 and acc is 0.74, learning_Rate is 0.20412329046744235\n",
            "epoch is 4000,loss is 0.612761914730072 and acc is 0.7466666666666667, learning_Rate is 0.2000400080016003\n",
            "epoch is 4100,loss is 0.6065987348556519 and acc is 0.7466666666666667, learning_Rate is 0.19611688566385566\n",
            "epoch is 4200,loss is 0.5996505618095398 and acc is 0.75, learning_Rate is 0.19234468166955185\n",
            "epoch is 4300,loss is 0.5931307077407837 and acc is 0.75, learning_Rate is 0.18871485185884126\n",
            "epoch is 4400,loss is 0.5892278552055359 and acc is 0.7633333333333333, learning_Rate is 0.18521948508983144\n",
            "epoch is 4500,loss is 0.5819454193115234 and acc is 0.76, learning_Rate is 0.18185124568103292\n",
            "epoch is 4600,loss is 0.6145097017288208 and acc is 0.71, learning_Rate is 0.1786033220217896\n",
            "epoch is 4700,loss is 0.5780497193336487 and acc is 0.7666666666666667, learning_Rate is 0.1754693805930865\n",
            "epoch is 4800,loss is 0.569278359413147 and acc is 0.7766666666666666, learning_Rate is 0.17244352474564578\n",
            "epoch is 4900,loss is 0.5639249682426453 and acc is 0.7766666666666666, learning_Rate is 0.16952025767079165\n",
            "epoch is 5000,loss is 0.5574464201927185 and acc is 0.7833333333333333, learning_Rate is 0.16669444907484582\n",
            "epoch is 5100,loss is 0.5519416928291321 and acc is 0.7933333333333333, learning_Rate is 0.16396130513198884\n",
            "epoch is 5200,loss is 0.5535367727279663 and acc is 0.7666666666666667, learning_Rate is 0.16131634134537828\n",
            "epoch is 5300,loss is 0.5607583522796631 and acc is 0.7633333333333333, learning_Rate is 0.15875535799333226\n",
            "epoch is 5400,loss is 0.5503582954406738 and acc is 0.7833333333333333, learning_Rate is 0.1562744178777934\n",
            "epoch is 5500,loss is 0.5393542647361755 and acc is 0.7866666666666666, learning_Rate is 0.15386982612709646\n",
            "epoch is 5600,loss is 0.5329571962356567 and acc is 0.79, learning_Rate is 0.15153811183512653\n",
            "epoch is 5700,loss is 0.5288791060447693 and acc is 0.7933333333333333, learning_Rate is 0.14927601134497687\n",
            "epoch is 5800,loss is 0.5239934921264648 and acc is 0.79, learning_Rate is 0.14708045300779526\n",
            "epoch is 5900,loss is 0.5198169946670532 and acc is 0.8066666666666666, learning_Rate is 0.14494854326714016\n",
            "epoch is 6000,loss is 0.5158973336219788 and acc is 0.8033333333333333, learning_Rate is 0.1428775539362766\n",
            "epoch is 6100,loss is 0.511676549911499 and acc is 0.8, learning_Rate is 0.1408649105507818\n",
            "epoch is 6200,loss is 0.508873701095581 and acc is 0.8, learning_Rate is 0.13890818169190167\n",
            "epoch is 6300,loss is 0.5066207647323608 and acc is 0.8, learning_Rate is 0.13700506918755992\n",
            "epoch is 6400,loss is 0.5035711526870728 and acc is 0.8033333333333333, learning_Rate is 0.13515339910798757\n",
            "epoch is 6500,loss is 0.5000303387641907 and acc is 0.8033333333333333, learning_Rate is 0.13335111348179757\n",
            "epoch is 6600,loss is 0.4963432848453522 and acc is 0.81, learning_Rate is 0.13159626266614027\n",
            "epoch is 6700,loss is 0.49158069491386414 and acc is 0.81, learning_Rate is 0.12988699831146902\n",
            "epoch is 6800,loss is 0.4880993068218231 and acc is 0.8133333333333334, learning_Rate is 0.12822156686754713\n",
            "epoch is 6900,loss is 0.4851331114768982 and acc is 0.8166666666666667, learning_Rate is 0.126598303582732\n",
            "epoch is 7000,loss is 0.4822831153869629 and acc is 0.8166666666666667, learning_Rate is 0.12501562695336915\n",
            "epoch is 7100,loss is 0.47653859853744507 and acc is 0.82, learning_Rate is 0.12347203358439313\n",
            "epoch is 7200,loss is 0.4730810523033142 and acc is 0.8266666666666667, learning_Rate is 0.12196609342602757\n",
            "epoch is 7300,loss is 0.46943262219429016 and acc is 0.83, learning_Rate is 0.12049644535486204\n",
            "epoch is 7400,loss is 0.4663485288619995 and acc is 0.83, learning_Rate is 0.11906179307060363\n",
            "epoch is 7500,loss is 0.46243906021118164 and acc is 0.83, learning_Rate is 0.11766090128250381\n",
            "epoch is 7600,loss is 0.4587547779083252 and acc is 0.83, learning_Rate is 0.11629259216187929\n",
            "epoch is 7700,loss is 0.4567149877548218 and acc is 0.8333333333333334, learning_Rate is 0.11495574203931487\n",
            "epoch is 7800,loss is 0.4516787827014923 and acc is 0.8333333333333334, learning_Rate is 0.11364927832708263\n",
            "epoch is 7900,loss is 0.4480490982532501 and acc is 0.8333333333333334, learning_Rate is 0.11237217664906168\n",
            "epoch is 8000,loss is 0.44538453221321106 and acc is 0.8366666666666667, learning_Rate is 0.11112345816201799\n",
            "epoch is 8100,loss is 0.44320130348205566 and acc is 0.83, learning_Rate is 0.10990218705352237\n",
            "epoch is 8200,loss is 0.43992045521736145 and acc is 0.8333333333333334, learning_Rate is 0.10870746820306555\n",
            "epoch is 8300,loss is 0.43689534068107605 and acc is 0.8333333333333334, learning_Rate is 0.1075384449940854\n",
            "epoch is 8400,loss is 0.4343522787094116 and acc is 0.8333333333333334, learning_Rate is 0.10639429726566654\n",
            "epoch is 8500,loss is 0.43137991428375244 and acc is 0.8366666666666667, learning_Rate is 0.10527423939362038\n",
            "epoch is 8600,loss is 0.4271961450576782 and acc is 0.84, learning_Rate is 0.10417751849150952\n",
            "epoch is 8700,loss is 0.4233061969280243 and acc is 0.8466666666666667, learning_Rate is 0.10310341272296113\n",
            "epoch is 8800,loss is 0.4204423427581787 and acc is 0.8533333333333334, learning_Rate is 0.1020512297173181\n",
            "epoch is 8900,loss is 0.4174029529094696 and acc is 0.85, learning_Rate is 0.10102030508132134\n",
            "epoch is 9000,loss is 0.4147900938987732 and acc is 0.8466666666666667, learning_Rate is 0.1000100010001\n",
            "epoch is 9100,loss is 0.41227588057518005 and acc is 0.8466666666666667, learning_Rate is 0.09901970492127933\n",
            "epoch is 9200,loss is 0.40997394919395447 and acc is 0.8433333333333334, learning_Rate is 0.09804882831650162\n",
            "epoch is 9300,loss is 0.4074179232120514 and acc is 0.85, learning_Rate is 0.09709680551509856\n",
            "epoch is 9400,loss is 0.40539616346359253 and acc is 0.85, learning_Rate is 0.09616309260505818\n",
            "epoch is 9500,loss is 0.4032837450504303 and acc is 0.85, learning_Rate is 0.09524716639679968\n",
            "epoch is 9600,loss is 0.40124276280403137 and acc is 0.85, learning_Rate is 0.09434852344560807\n",
            "epoch is 9700,loss is 0.39924266934394836 and acc is 0.8533333333333334, learning_Rate is 0.09346667912889055\n",
            "epoch is 9800,loss is 0.39738473296165466 and acc is 0.8533333333333334, learning_Rate is 0.09260116677470137\n",
            "epoch is 9900,loss is 0.3957523703575134 and acc is 0.8533333333333334, learning_Rate is 0.09175153683824203\n",
            "epoch is 10000,loss is 0.39393723011016846 and acc is 0.8533333333333334, learning_Rate is 0.09091735612328393\n",
            "epoch is 10100,loss is 0.39238011837005615 and acc is 0.8533333333333334, learning_Rate is 0.09009820704567979\n",
            "epoch is 10200,loss is 0.3908596932888031 and acc is 0.8533333333333334, learning_Rate is 0.0892936869363336\n",
            "epoch is 10300,loss is 0.3893998861312866 and acc is 0.8533333333333334, learning_Rate is 0.08850340738118417\n",
            "epoch is 10400,loss is 0.38795602321624756 and acc is 0.8566666666666667, learning_Rate is 0.08772699359592946\n",
            "epoch is 10500,loss is 0.38645482063293457 and acc is 0.8566666666666667, learning_Rate is 0.08696408383337681\n",
            "epoch is 10600,loss is 0.38511544466018677 and acc is 0.8566666666666667, learning_Rate is 0.08621432882145012\n",
            "epoch is 10700,loss is 0.3837134838104248 and acc is 0.8566666666666667, learning_Rate is 0.08547739123001966\n",
            "epoch is 10800,loss is 0.38241881132125854 and acc is 0.8566666666666667, learning_Rate is 0.08475294516484448\n",
            "epoch is 10900,loss is 0.3811270296573639 and acc is 0.8566666666666667, learning_Rate is 0.08404067568703252\n",
            "epoch is 11000,loss is 0.3798602819442749 and acc is 0.86, learning_Rate is 0.08334027835652971\n",
            "epoch is 11100,loss is 0.3786526024341583 and acc is 0.8633333333333333, learning_Rate is 0.08265145879824778\n",
            "epoch is 11200,loss is 0.3774968981742859 and acc is 0.8633333333333333, learning_Rate is 0.08197393228953193\n",
            "epoch is 11300,loss is 0.37633392214775085 and acc is 0.8633333333333333, learning_Rate is 0.08130742336775348\n",
            "epoch is 11400,loss is 0.37515100836753845 and acc is 0.8633333333333333, learning_Rate is 0.08065166545689167\n",
            "epoch is 11500,loss is 0.37407663464546204 and acc is 0.8633333333333333, learning_Rate is 0.08000640051204096\n",
            "epoch is 11600,loss is 0.37293732166290283 and acc is 0.8633333333333333, learning_Rate is 0.07937137868084769\n",
            "epoch is 11700,loss is 0.37189945578575134 and acc is 0.8633333333333333, learning_Rate is 0.07874635798094339\n",
            "epoch is 11800,loss is 0.3708359897136688 and acc is 0.8633333333333333, learning_Rate is 0.07813110399249942\n",
            "epoch is 11900,loss is 0.3694155812263489 and acc is 0.8633333333333333, learning_Rate is 0.07752538956508256\n",
            "epoch is 12000,loss is 0.3679625988006592 and acc is 0.8633333333333333, learning_Rate is 0.07692899453804139\n",
            "epoch is 12100,loss is 0.3665575683116913 and acc is 0.8733333333333333, learning_Rate is 0.07634170547370028\n",
            "epoch is 12200,loss is 0.3647244870662689 and acc is 0.87, learning_Rate is 0.07576331540268202\n",
            "epoch is 12300,loss is 0.3636423349380493 and acc is 0.87, learning_Rate is 0.07519362358072036\n",
            "epoch is 12400,loss is 0.3625868558883667 and acc is 0.87, learning_Rate is 0.07463243525636241\n",
            "epoch is 12500,loss is 0.36159276962280273 and acc is 0.87, learning_Rate is 0.07407956144899622\n",
            "epoch is 12600,loss is 0.36059990525245667 and acc is 0.87, learning_Rate is 0.07353481873667181\n",
            "epoch is 12700,loss is 0.3595777750015259 and acc is 0.87, learning_Rate is 0.07299802905321556\n",
            "epoch is 12800,loss is 0.3585244119167328 and acc is 0.87, learning_Rate is 0.07246901949416625\n",
            "epoch is 12900,loss is 0.35740455985069275 and acc is 0.8733333333333333, learning_Rate is 0.07194762213108856\n",
            "epoch is 13000,loss is 0.3563275635242462 and acc is 0.8733333333333333, learning_Rate is 0.07143367383384527\n",
            "epoch is 13100,loss is 0.3552595376968384 and acc is 0.8766666666666667, learning_Rate is 0.07092701610043266\n",
            "epoch is 13200,loss is 0.35414618253707886 and acc is 0.88, learning_Rate is 0.07042749489400663\n",
            "epoch is 13300,loss is 0.35308822989463806 and acc is 0.88, learning_Rate is 0.06993496048674733\n",
            "epoch is 13400,loss is 0.3520689308643341 and acc is 0.88, learning_Rate is 0.06944926731022988\n",
            "epoch is 13500,loss is 0.3511107563972473 and acc is 0.88, learning_Rate is 0.06897027381198703\n",
            "epoch is 13600,loss is 0.350180983543396 and acc is 0.88, learning_Rate is 0.06849784231796699\n",
            "epoch is 13700,loss is 0.34926101565361023 and acc is 0.88, learning_Rate is 0.06803183890060549\n",
            "epoch is 13800,loss is 0.34842509031295776 and acc is 0.88, learning_Rate is 0.06757213325224677\n",
            "epoch is 13900,loss is 0.3476070761680603 and acc is 0.88, learning_Rate is 0.06711859856366198\n",
            "epoch is 14000,loss is 0.346800297498703 and acc is 0.88, learning_Rate is 0.06667111140742715\n",
            "epoch is 14100,loss is 0.3459831774234772 and acc is 0.88, learning_Rate is 0.0662295516259355\n",
            "epoch is 14200,loss is 0.34463492035865784 and acc is 0.88, learning_Rate is 0.06579380222383052\n",
            "epoch is 14300,loss is 0.34354570508003235 and acc is 0.8766666666666667, learning_Rate is 0.06536374926465782\n",
            "epoch is 14400,loss is 0.34259921312332153 and acc is 0.8766666666666667, learning_Rate is 0.06493928177154361\n",
            "epoch is 14500,loss is 0.34168702363967896 and acc is 0.8766666666666667, learning_Rate is 0.06452029163171817\n",
            "epoch is 14600,loss is 0.3408351242542267 and acc is 0.8833333333333333, learning_Rate is 0.06410667350471184\n",
            "epoch is 14700,loss is 0.34002426266670227 and acc is 0.8833333333333333, learning_Rate is 0.0636983247340595\n",
            "epoch is 14800,loss is 0.33921700716018677 and acc is 0.8833333333333333, learning_Rate is 0.06329514526235838\n",
            "epoch is 14900,loss is 0.3384312093257904 and acc is 0.8833333333333333, learning_Rate is 0.06289703754953141\n",
            "epoch is 15000,loss is 0.33766910433769226 and acc is 0.8833333333333333, learning_Rate is 0.06250390649415588\n",
            "epoch is 15100,loss is 0.33692666888237 and acc is 0.8833333333333333, learning_Rate is 0.06211565935772408\n",
            "epoch is 15200,loss is 0.3361600935459137 and acc is 0.8833333333333333, learning_Rate is 0.061732205691709376\n",
            "epoch is 15300,loss is 0.3354046046733856 and acc is 0.8833333333333333, learning_Rate is 0.061353457267317016\n",
            "epoch is 15400,loss is 0.33452752232551575 and acc is 0.8833333333333333, learning_Rate is 0.06097932800780535\n",
            "epoch is 15500,loss is 0.33371683955192566 and acc is 0.8833333333333333, learning_Rate is 0.060609733923268065\n",
            "epoch is 15600,loss is 0.33295533061027527 and acc is 0.8833333333333333, learning_Rate is 0.060244593047773964\n",
            "epoch is 15700,loss is 0.3321937918663025 and acc is 0.88, learning_Rate is 0.0598838253787652\n",
            "epoch is 15800,loss is 0.3314536511898041 and acc is 0.88, learning_Rate is 0.059527352818620156\n",
            "epoch is 15900,loss is 0.33073267340660095 and acc is 0.8833333333333333, learning_Rate is 0.05917509911829102\n",
            "epoch is 16000,loss is 0.3300307095050812 and acc is 0.8833333333333333, learning_Rate is 0.058826989822930754\n",
            "epoch is 16100,loss is 0.3293536901473999 and acc is 0.8833333333333333, learning_Rate is 0.058482952219428036\n",
            "epoch is 16200,loss is 0.3286837339401245 and acc is 0.8833333333333333, learning_Rate is 0.05814291528577242\n",
            "epoch is 16300,loss is 0.328014612197876 and acc is 0.8866666666666667, learning_Rate is 0.05780680964217585\n",
            "epoch is 16400,loss is 0.32736822962760925 and acc is 0.8866666666666667, learning_Rate is 0.05747456750387953\n",
            "epoch is 16500,loss is 0.32672616839408875 and acc is 0.8866666666666667, learning_Rate is 0.05714612263557918\n",
            "epoch is 16600,loss is 0.32609257102012634 and acc is 0.8866666666666667, learning_Rate is 0.05682141030740383\n",
            "epoch is 16700,loss is 0.3254590630531311 and acc is 0.8866666666666667, learning_Rate is 0.056500367252387135\n",
            "epoch is 16800,loss is 0.32484278082847595 and acc is 0.8866666666666667, learning_Rate is 0.05618293162537221\n",
            "epoch is 16900,loss is 0.32422617077827454 and acc is 0.8866666666666667, learning_Rate is 0.055869042963294036\n",
            "epoch is 17000,loss is 0.3236096203327179 and acc is 0.8866666666666667, learning_Rate is 0.055558642146785936\n",
            "epoch is 17100,loss is 0.32298752665519714 and acc is 0.89, learning_Rate is 0.05525167136305873\n",
            "epoch is 17200,loss is 0.3223704993724823 and acc is 0.89, learning_Rate is 0.05494807407000384\n",
            "epoch is 17300,loss is 0.32175299525260925 and acc is 0.89, learning_Rate is 0.0546477949614733\n",
            "epoch is 17400,loss is 0.32113489508628845 and acc is 0.89, learning_Rate is 0.05435077993369205\n",
            "epoch is 17500,loss is 0.32054778933525085 and acc is 0.89, learning_Rate is 0.05405697605275961\n",
            "epoch is 17600,loss is 0.31998053193092346 and acc is 0.89, learning_Rate is 0.05376633152320017\n",
            "epoch is 17700,loss is 0.3194044530391693 and acc is 0.89, learning_Rate is 0.05347879565752179\n",
            "epoch is 17800,loss is 0.3188484311103821 and acc is 0.89, learning_Rate is 0.05319431884674717\n",
            "epoch is 17900,loss is 0.3182903528213501 and acc is 0.89, learning_Rate is 0.05291285253187999\n",
            "epoch is 18000,loss is 0.31771233677864075 and acc is 0.89, learning_Rate is 0.05263434917627244\n",
            "epoch is 18100,loss is 0.31706055998802185 and acc is 0.89, learning_Rate is 0.05235876223886067\n",
            "epoch is 18200,loss is 0.3165101110935211 and acc is 0.89, learning_Rate is 0.052086046148236885\n",
            "epoch is 18300,loss is 0.31597229838371277 and acc is 0.89, learning_Rate is 0.05181615627752734\n",
            "epoch is 18400,loss is 0.3154459297657013 and acc is 0.89, learning_Rate is 0.05154904892004742\n",
            "epoch is 18500,loss is 0.314907431602478 and acc is 0.89, learning_Rate is 0.051284681265705935\n",
            "epoch is 18600,loss is 0.3143664598464966 and acc is 0.89, learning_Rate is 0.05102301137813154\n",
            "epoch is 18700,loss is 0.3138290345668793 and acc is 0.89, learning_Rate is 0.050763998172496064\n",
            "epoch is 18800,loss is 0.3133033514022827 and acc is 0.89, learning_Rate is 0.0505076013940098\n",
            "epoch is 18900,loss is 0.31278863549232483 and acc is 0.89, learning_Rate is 0.05025378159706518\n",
            "epoch is 19000,loss is 0.3122793436050415 and acc is 0.89, learning_Rate is 0.050002500125006254\n",
            "epoch is 19100,loss is 0.31177979707717896 and acc is 0.8933333333333333, learning_Rate is 0.04975371909050202\n",
            "epoch is 19200,loss is 0.31129154562950134 and acc is 0.8933333333333333, learning_Rate is 0.04950740135650279\n",
            "epoch is 19300,loss is 0.3108011484146118 and acc is 0.8933333333333333, learning_Rate is 0.0492635105177595\n",
            "epoch is 19400,loss is 0.3103165328502655 and acc is 0.8933333333333333, learning_Rate is 0.049022010882886415\n",
            "epoch is 19500,loss is 0.3098331689834595 and acc is 0.8933333333333333, learning_Rate is 0.048782867456949125\n",
            "epoch is 19600,loss is 0.3093582093715668 and acc is 0.8933333333333333, learning_Rate is 0.048546045924559446\n",
            "epoch is 19700,loss is 0.3088866174221039 and acc is 0.8933333333333333, learning_Rate is 0.04831151263346055\n",
            "epoch is 19800,loss is 0.3084191381931305 and acc is 0.8933333333333333, learning_Rate is 0.04807923457858551\n",
            "epoch is 19900,loss is 0.30795222520828247 and acc is 0.8933333333333333, learning_Rate is 0.047849179386573515\n",
            "epoch is 20000,loss is 0.3074936270713806 and acc is 0.8933333333333333, learning_Rate is 0.047621315300728606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SChWlYdleBbu"
      },
      "source": [
        "class ADAGRAD:\r\n",
        "\r\n",
        "  def __init__(self,learning_rate=1,rate_decay=0.,epsilon=1e-7):\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.rate_decay = rate_decay\r\n",
        "    self.epsilon = epsilon\r\n",
        "    self.iterations = 0\r\n",
        "    self.current_learning_rate = learning_rate\r\n",
        "\r\n",
        "  def update_learning_rate(self):\r\n",
        "    self.current_learning_rate = self.learning_rate * (1. / (1. + self.rate_decay * self.iterations))\r\n",
        "\r\n",
        "  def update_params(self,layer):\r\n",
        "    if not hasattr(layer,'previous_weights'):\r\n",
        "      layer.previous_weights = np.zeros_like(layer.weights)\r\n",
        "      layer.previous_biases = np.zeros_like(layer.biases)\r\n",
        "    \r\n",
        "  \r\n",
        "    layer.previous_weights += layer.dweights ** 2\r\n",
        "    layer.previous_biases += layer.dbiases ** 2\r\n",
        "\r\n",
        "    layer.weights -= self.current_learning_rate * layer.dweights / (np.sqrt(layer.previous_weights) + self.epsilon)\r\n",
        "    layer.biases -= self.current_learning_rate * layer.dbiases / (np.sqrt(layer.previous_biases) + self.epsilon)\r\n",
        "\r\n",
        "  def update_iterations(self):\r\n",
        "    self.iterations  += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ktfJpugfNH",
        "outputId": "41a84845-485d-4f5e-becf-e4451bb4558d"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,3)\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "optimizers = ADAGRAD(learning_rate = 1,rate_decay=1e-4)\r\n",
        "for epoch in range(10001):\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  loss = activation2.forward(dense2.output,y)\r\n",
        "\r\n",
        "  prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "  acc = np.mean(prediction==y)\r\n",
        "\r\n",
        "  if not epoch % 500 :\r\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epoch,loss,acc,optimizers.current_learning_rate))\r\n",
        "\r\n",
        "  activation2.backward(activation2.output,y)\r\n",
        "\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizers.update_learning_rate()\r\n",
        "  optimizers.update_params(dense1)\r\n",
        "  optimizers.update_params(dense2)\r\n",
        "  optimizers.update_iterations()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.0986255407333374 and acc is 0.2733333333333333, learning_Rate is 1\n",
            "epoch is 500,loss is 0.9134111404418945 and acc is 0.51, learning_Rate is 0.9524716639679969\n",
            "epoch is 1000,loss is 0.8481355905532837 and acc is 0.5266666666666666, learning_Rate is 0.9091735612328392\n",
            "epoch is 1500,loss is 0.8075557351112366 and acc is 0.57, learning_Rate is 0.8696408383337683\n",
            "epoch is 2000,loss is 0.777934730052948 and acc is 0.6066666666666667, learning_Rate is 0.8334027835652972\n",
            "epoch is 2500,loss is 0.7559630274772644 and acc is 0.6266666666666667, learning_Rate is 0.8000640051204096\n",
            "epoch is 3000,loss is 0.7528643012046814 and acc is 0.5766666666666667, learning_Rate is 0.7692899453804138\n",
            "epoch is 3500,loss is 0.7370365262031555 and acc is 0.6233333333333333, learning_Rate is 0.7407956144899621\n",
            "epoch is 4000,loss is 0.7250846028327942 and acc is 0.6166666666666667, learning_Rate is 0.7143367383384527\n",
            "epoch is 4500,loss is 0.7149375677108765 and acc is 0.6333333333333333, learning_Rate is 0.6897027381198704\n",
            "epoch is 5000,loss is 0.7073140740394592 and acc is 0.63, learning_Rate is 0.6667111140742716\n",
            "epoch is 5500,loss is 0.698095977306366 and acc is 0.6433333333333333, learning_Rate is 0.6452029163171817\n",
            "epoch is 6000,loss is 0.6936862468719482 and acc is 0.6166666666666667, learning_Rate is 0.6250390649415589\n",
            "epoch is 6500,loss is 0.6881486177444458 and acc is 0.6233333333333333, learning_Rate is 0.6060973392326807\n",
            "epoch is 7000,loss is 0.6844022870063782 and acc is 0.6233333333333333, learning_Rate is 0.5882698982293076\n",
            "epoch is 7500,loss is 0.6800320148468018 and acc is 0.6466666666666666, learning_Rate is 0.5714612263557918\n",
            "epoch is 8000,loss is 0.6760106682777405 and acc is 0.6433333333333333, learning_Rate is 0.5555864214678593\n",
            "epoch is 8500,loss is 0.6722920536994934 and acc is 0.6366666666666667, learning_Rate is 0.5405697605275961\n",
            "epoch is 9000,loss is 0.6684922575950623 and acc is 0.6433333333333333, learning_Rate is 0.5263434917627243\n",
            "epoch is 9500,loss is 0.6647104620933533 and acc is 0.66, learning_Rate is 0.5128468126570593\n",
            "epoch is 10000,loss is 0.6561006307601929 and acc is 0.67, learning_Rate is 0.5000250012500626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGBl9I8_jrew"
      },
      "source": [
        "class ADAM:\r\n",
        "\r\n",
        "  def __init__(self,learning_rate=0.01,decay_rate=0.,epsilon=1e-7,beta1=0.9,beta2=0.999):\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.decay_rate = decay_rate\r\n",
        "    self.epsilon = epsilon \r\n",
        "    self.beta1 = beta1\r\n",
        "    self.beta2 = beta2\r\n",
        "    self.iterations = 0\r\n",
        "    self.current_learning_rate = learning_rate\r\n",
        "\r\n",
        "  def update_learning_rate(self):\r\n",
        "\r\n",
        "    self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay_rate * self.iterations))\r\n",
        "\r\n",
        "  def update_params(self,layer):\r\n",
        "    if not hasattr(layer,'previous_weights'):\r\n",
        "      layer.previous_weights = np.zeros_like(layer.weights)\r\n",
        "      layer.previous_biases = np.zeros_like(layer.biases)\r\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\r\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\r\n",
        "\r\n",
        "    layer.weight_momentums = self.beta1 * layer.weight_momentums + (1-self.beta1) * layer.dweights\r\n",
        "    layer.bias_momentums = self.beta1 * layer.bias_momentums + (1 -self.beta1) * layer.dbiases\r\n",
        "    \r\n",
        "    weight_momentums_hat = layer.weight_momentums / ( 1 - self.beta1 ** (self.iterations + 1))\r\n",
        "    bias_momentums_hat = layer.bias_momentums / ( 1 - self.beta1 ** (self.iterations + 1))\r\n",
        "\r\n",
        "    layer.previous_weights = self.beta2 * layer.previous_weights + (1 - self.beta2) * layer.dweights ** 2\r\n",
        "    layer.previous_biases = self.beta2 * layer.previous_biases + (1 - self.beta2) * layer.dbiases ** 2\r\n",
        "\r\n",
        "    previous_weights_hat = layer.previous_weights / ( 1 - self.beta2 ** (self.iterations + 1))\r\n",
        "    previous_biases_hat = layer.previous_biases / ( 1 - self.beta2 ** (self.iterations + 1))\r\n",
        "\r\n",
        "    layer.weights -= self.current_learning_rate * weight_momentums_hat / (np.sqrt(previous_weights_hat) + self.epsilon)\r\n",
        "    layer.biases -= self.current_learning_rate * bias_momentums_hat / (np.sqrt(previous_biases_hat) + self.epsilon)\r\n",
        "\r\n",
        "\r\n",
        "  def update_iterations(self):\r\n",
        "    self.iterations += 1\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnvUWpCVfsUv",
        "outputId": "b8b1ae9b-c2e8-47f0-9dc1-de9b041256d5"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,3)\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "optimizers = ADAM(learning_rate=0.02,decay_rate=1e-5)\r\n",
        "for epoch in range(10001):\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  loss = activation2.forward(dense2.output,y)\r\n",
        "\r\n",
        "  prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "  acc = np.mean(prediction==y)\r\n",
        "\r\n",
        "  if not epoch % 500 :\r\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epoch,loss,acc,optimizers.current_learning_rate))\r\n",
        "\r\n",
        "  activation2.backward(activation2.output,y)\r\n",
        "\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizers.update_learning_rate()\r\n",
        "  optimizers.update_params(dense1)\r\n",
        "  optimizers.update_params(dense2)\r\n",
        "  optimizers.update_iterations()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.0985994338989258 and acc is 0.31333333333333335, learning_Rate is 0.02\n",
            "epoch is 500,loss is 0.5556354522705078 and acc is 0.7533333333333333, learning_Rate is 0.01990069552930875\n",
            "epoch is 1000,loss is 0.5088706612586975 and acc is 0.78, learning_Rate is 0.019802176259170884\n",
            "epoch is 1500,loss is 0.48465219140052795 and acc is 0.7866666666666666, learning_Rate is 0.019704627631799327\n",
            "epoch is 2000,loss is 0.473643958568573 and acc is 0.79, learning_Rate is 0.019608035372895814\n",
            "epoch is 2500,loss is 0.46621784567832947 and acc is 0.79, learning_Rate is 0.019512385486687673\n",
            "epoch is 3000,loss is 0.46129754185676575 and acc is 0.7833333333333333, learning_Rate is 0.01941766424916747\n",
            "epoch is 3500,loss is 0.4574313759803772 and acc is 0.7966666666666666, learning_Rate is 0.019323858201528515\n",
            "epoch is 4000,loss is 0.45426371693611145 and acc is 0.7966666666666666, learning_Rate is 0.019230954143789846\n",
            "epoch is 4500,loss is 0.4529135525226593 and acc is 0.7933333333333333, learning_Rate is 0.0191389391286041\n",
            "epoch is 5000,loss is 0.45148807764053345 and acc is 0.7966666666666666, learning_Rate is 0.01904780045524243\n",
            "epoch is 5500,loss is 0.4462719261646271 and acc is 0.7933333333333333, learning_Rate is 0.018957525663750367\n",
            "epoch is 6000,loss is 0.4458947777748108 and acc is 0.7966666666666666, learning_Rate is 0.018868102529269144\n",
            "epoch is 6500,loss is 0.44390422105789185 and acc is 0.7966666666666666, learning_Rate is 0.01877951905651696\n",
            "epoch is 7000,loss is 0.4423394799232483 and acc is 0.7933333333333333, learning_Rate is 0.018691763474424996\n",
            "epoch is 7500,loss is 0.4403839111328125 and acc is 0.8066666666666666, learning_Rate is 0.018604824230923075\n",
            "epoch is 8000,loss is 0.43788549304008484 and acc is 0.8, learning_Rate is 0.01851868998787026\n",
            "epoch is 8500,loss is 0.43554869294166565 and acc is 0.7933333333333333, learning_Rate is 0.018433349616125496\n",
            "epoch is 9000,loss is 0.4378555715084076 and acc is 0.8, learning_Rate is 0.018348792190754044\n",
            "epoch is 9500,loss is 0.4323051869869232 and acc is 0.8033333333333333, learning_Rate is 0.018265006986365174\n",
            "epoch is 10000,loss is 0.43018385767936707 and acc is 0.8, learning_Rate is 0.018181983472577025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfwCVoucfwTt",
        "outputId": "76cc5e63-073d-42aa-de51-6a70f5c07fb0"
      },
      "source": [
        "\r\n",
        "X, y = spiral_data(samples=100, classes=3)\r\n",
        "\r\n",
        "dense1 = Dense(2,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,3)\r\n",
        "\r\n",
        "activation2 = Softmax_Activation_Crossentropy()\r\n",
        "\r\n",
        "optimizers = ADAM(learning_rate=0.05,decay_rate=5e-7)\r\n",
        "for epoch in range(10001):\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  loss = activation2.forward(dense2.output,y)\r\n",
        "\r\n",
        "  prediction = np.argmax(activation2.output,axis=1)\r\n",
        "\r\n",
        "  acc = np.mean(prediction==y)\r\n",
        "\r\n",
        "  if not epoch % 500 :\r\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epoch,loss,acc,optimizers.current_learning_rate))\r\n",
        "\r\n",
        "  activation2.backward(activation2.output,y)\r\n",
        "\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizers.update_learning_rate()\r\n",
        "  optimizers.update_params(dense1)\r\n",
        "  optimizers.update_params(dense2)\r\n",
        "  optimizers.update_iterations()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.0985811948776245 and acc is 0.35333333333333333, learning_Rate is 0.05\n",
            "epoch is 500,loss is 0.3052339255809784 and acc is 0.89, learning_Rate is 0.049987528111736124\n",
            "epoch is 1000,loss is 0.22664718329906464 and acc is 0.9133333333333333, learning_Rate is 0.049975037468784345\n",
            "epoch is 1500,loss is 0.2003358006477356 and acc is 0.9133333333333333, learning_Rate is 0.04996255306647668\n",
            "epoch is 2000,loss is 0.1966646909713745 and acc is 0.9166666666666666, learning_Rate is 0.04995007490013731\n",
            "epoch is 2500,loss is 0.17819903790950775 and acc is 0.9233333333333333, learning_Rate is 0.04993760296509512\n",
            "epoch is 3000,loss is 0.1651885211467743 and acc is 0.93, learning_Rate is 0.049925137256683606\n",
            "epoch is 3500,loss is 0.15123620629310608 and acc is 0.94, learning_Rate is 0.049912677770240964\n",
            "epoch is 4000,loss is 0.14774049818515778 and acc is 0.9366666666666666, learning_Rate is 0.049900224501110035\n",
            "epoch is 4500,loss is 0.13810968399047852 and acc is 0.9466666666666667, learning_Rate is 0.04988777744463829\n",
            "epoch is 5000,loss is 0.13251563906669617 and acc is 0.94, learning_Rate is 0.04987533659617785\n",
            "epoch is 5500,loss is 0.13761824369430542 and acc is 0.9466666666666667, learning_Rate is 0.049862901951085496\n",
            "epoch is 6000,loss is 0.13478459417819977 and acc is 0.95, learning_Rate is 0.04985047350472258\n",
            "epoch is 6500,loss is 0.12839946150779724 and acc is 0.9366666666666666, learning_Rate is 0.049838051252455155\n",
            "epoch is 7000,loss is 0.12142231315374374 and acc is 0.96, learning_Rate is 0.04982563518965381\n",
            "epoch is 7500,loss is 0.11506380885839462 and acc is 0.9533333333333334, learning_Rate is 0.0498132253116938\n",
            "epoch is 8000,loss is 0.1133321151137352 and acc is 0.9566666666666667, learning_Rate is 0.04980082161395499\n",
            "epoch is 8500,loss is 0.11199212074279785 and acc is 0.9666666666666667, learning_Rate is 0.049788424091821805\n",
            "epoch is 9000,loss is 0.10586090385913849 and acc is 0.95, learning_Rate is 0.04977603274068329\n",
            "epoch is 9500,loss is 0.10828354954719543 and acc is 0.9733333333333334, learning_Rate is 0.0497636475559331\n",
            "epoch is 10000,loss is 0.10528705269098282 and acc is 0.97, learning_Rate is 0.04975126853296942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dztbQbg1T9V"
      },
      "source": [
        "<h2> L1 and L2 regularizers </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ph1I301h6z"
      },
      "source": [
        "class Dense:\r\n",
        "\r\n",
        "  def __init__(self,n_samples,n_neurons,weight_regularizer_l2=0,bias_regularizer_l2=0,weight_regularizer_l1=0,bias_regularizer_l1=0):\r\n",
        "    self.weights = 0.01 * np.random.randn(n_samples,n_neurons)\r\n",
        "    self.biases = np.zeros((1,n_neurons))\r\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\r\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\r\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\r\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\r\n",
        "    \r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.dot(inputs,self.weights) + self.biases\r\n",
        "    self.inputs = inputs\r\n",
        "\r\n",
        "  def backward(self,dvalues):\r\n",
        "\r\n",
        "    self.dweights = np.dot(self.inputs.T,dvalues)\r\n",
        "    self.dbiases = np.sum(dvalues,axis=0,keepdims=True)\r\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\r\n",
        "\r\n",
        "    # gradients for l2 or l1 regularizer if used \r\n",
        "\r\n",
        "    if self.weight_regularizer_l2 > 0 :\r\n",
        "      res = 2 * self.weight_regularizer_l2 * self.weights\r\n",
        "      self.dweights += res\r\n",
        "    if self.bias_regularizer_l2 > 0 :\r\n",
        "      res = 2 * self.bias_regularizer_l2 * self.biases \r\n",
        "      self.dbiases += res\r\n",
        "    if self.weight_regularizer_l1 > 0:\r\n",
        "      temp = np.ones_like(self.weights)\r\n",
        "      temp[self.weights < 0] = -1\r\n",
        "      self.dweights += self.weight_regularizer_l1 * temp\r\n",
        "    if self.bias_regularizer_l1 > 0:\r\n",
        "      temp = np.ones_like(self.biases)\r\n",
        "      temp[self.biases < 0 ] = -1\r\n",
        "      self.dbiases += self.bias_regularizer_l1 * temp\r\n",
        "\r\n",
        "class ActivationRelu:\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = np.maximum(0,inputs)\r\n",
        "    self.inputs = inputs\r\n",
        "  \r\n",
        "  def backward(self,dvalues):\r\n",
        "\r\n",
        "    self.drelu = dvalues.copy()\r\n",
        "    self.drelu[self.inputs<=0] = 0\r\n",
        "\r\n",
        "\r\n",
        "class ActivationSoftmax:\r\n",
        "\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    \r\n",
        "    exp = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    self.output = exp / np.sum(exp,axis=1,keepdims=True) # row wise sum for each sample output shape will be output = (n_samples,n_classes)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Loss:\r\n",
        "\r\n",
        "  def regularizer_loss(self,layer):\r\n",
        "\r\n",
        "    regularize_loss = 0\r\n",
        "\r\n",
        "    if layer.weight_regularizer_l2 > 0:\r\n",
        "      regularize_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\r\n",
        "\r\n",
        "    if layer.bias_regularizer_l2 > 0:\r\n",
        "      regularize_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\r\n",
        "\r\n",
        "    if layer.weight_regularizer_l1 > 0:\r\n",
        "      regularize_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\r\n",
        "\r\n",
        "    if layer.bias_regularizer_l1 > 0:\r\n",
        "      regularize_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\r\n",
        "\r\n",
        "\r\n",
        "    return regularize_loss\r\n",
        "\r\n",
        "\r\n",
        "  def calculate(self,outputs,y):\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    loss_value = self.forward(outputs,y)\r\n",
        "\r\n",
        "    mean_value = np.mean(loss_value)\r\n",
        "  \r\n",
        "    return mean_value\r\n",
        "\r\n",
        "class Categorical_Crossentropy(Loss):\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "    samples = len(y_pred) \r\n",
        "\r\n",
        "    y_pred = np.clip(y_pred,1e-7, 1 - 1e-7)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 1:\r\n",
        "      y_pred  = y_pred[range(samples),y_true]\r\n",
        "\r\n",
        "    elif len(y_true.shape) == 2:\r\n",
        "      y_pred = np.sum(y_pred*y_true,axis=1)\r\n",
        "\r\n",
        "    probabilities = -np.log(y_pred)\r\n",
        "\r\n",
        "    return probabilities\r\n",
        "\r\n",
        "\r\n",
        "class Softmax_Activation_Crossentropy:\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.activation = ActivationSoftmax()\r\n",
        "    self.loss = Categorical_Crossentropy()\r\n",
        "  \r\n",
        "  def forward(self,inputs,y_true):\r\n",
        "\r\n",
        "    self.activation.forward(inputs)\r\n",
        "    \r\n",
        "    self.output = self.activation.output\r\n",
        "\r\n",
        "    \r\n",
        "    return self.loss.calculate(self.output,y_true)\r\n",
        "\r\n",
        "\r\n",
        "  def backward(self,dvalues,y_true):\r\n",
        "\r\n",
        "    samples = len(dvalues)\r\n",
        "\r\n",
        "    if len(y_true.shape) == 2:\r\n",
        "      y_true = np.argmax(y_true,axis=1)\r\n",
        "\r\n",
        "    self.dinputs = dvalues.copy()\r\n",
        "    # derivative of softmax is y_i_j - y_hat_i_j\r\n",
        "    self.dinputs[range(samples),y_true] -= 1\r\n",
        "\r\n",
        "    self.dinputs = self.dinputs / samples\r\n",
        "\r\n",
        "    \r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpzUQ8VCYJJ3"
      },
      "source": [
        "<h2> Dropout </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu8OHdsbYK2_"
      },
      "source": [
        "# class to implement dropout\r\n",
        "class Dropout:\r\n",
        "\r\n",
        "  def __init__(self,rate):\r\n",
        "    self.rate = 1 - rate # rate to keep percent of neurons enable\r\n",
        "\r\n",
        "  def forward(self,inputs):\r\n",
        "    \r\n",
        "\r\n",
        "    self.binary_mask = np.random.binomial(1,self.rate,size=inputs.shape) / self.rate\r\n",
        "    self.output = inputs * self.binary_mask\r\n",
        "\r\n",
        "  def backward(self,dinputs):\r\n",
        "\r\n",
        "    self.dvalues = dinputs * self.binary_mask\r\n",
        "\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-nPAh0TbY4r",
        "outputId": "5d7b7263-48fd-4105-bb52-f85996e4f0bc"
      },
      "source": [
        "\n",
        "X, y = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "dense1 = Dense(2,64,weight_regularizer_l2=5e-4,bias_regularizer_l2=5e-4)\n",
        "\n",
        "dropout1 = Dropout(0.1)\n",
        "\n",
        "activation1 = ActivationRelu()\n",
        "\n",
        "dense2 = Dense(64,3)\n",
        "\n",
        "\n",
        "loss_activation = Softmax_Activation_Crossentropy()\n",
        "\n",
        "\n",
        "optimizers = ADAM(learning_rate=0.05, decay_rate=5e-5)\n",
        "\n",
        "\n",
        "for epochs in range(5001):\n",
        "\n",
        "  dense1.forward(X)\n",
        "\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  dropout1.forward(activation1.output)\n",
        "\n",
        "  dense2.forward(dropout1.output)\n",
        "\n",
        "\n",
        "\n",
        "  data_loss = loss_activation.forward(dense2.output,y)\n",
        "\n",
        "  regularization_loss = loss_activation.loss.regularizer_loss(dense1) + loss_activation.loss.regularizer_loss(dense2)\n",
        "\n",
        "  total_loss = data_loss + regularization_loss\n",
        "\n",
        "  predictions = np.argmax(loss_activation.output,axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y,axis=1)\n",
        "  acc = np.mean(predictions==y)\n",
        "  if not epochs % 200:\n",
        "    print('epoch is {},loss is {} and acc is {}, learning_Rate is {}'.format(epochs,total_loss,acc,optimizers.current_learning_rate))\n",
        "\n",
        "\n",
        "  loss_activation.backward(loss_activation.output,y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  #activation2.backward(dense3.dinputs)\n",
        "  dropout1.backward(dense2.dinputs)\n",
        "\n",
        "  activation1.backward(dropout1.dvalues)\n",
        "\n",
        "  dense1.backward(activation1.drelu)\n",
        "\n",
        "  optimizers.update_learning_rate()\n",
        "  optimizers.update_params(dense1)\n",
        "  optimizers.update_params(dense2)\n",
        "  optimizers.update_iterations()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch is 0,loss is 1.0986000350825489 and acc is 0.37466666666666665, learning_Rate is 0.05\n",
            "epoch is 200,loss is 0.8441108603477478 and acc is 0.6243333333333333, learning_Rate is 0.049507401356502806\n",
            "epoch is 400,loss is 0.7940272130966186 and acc is 0.651, learning_Rate is 0.04902201088288642\n",
            "epoch is 600,loss is 0.7521327548027039 and acc is 0.6726666666666666, learning_Rate is 0.04854604592455945\n",
            "epoch is 800,loss is 0.7632229690551757 and acc is 0.6756666666666666, learning_Rate is 0.04807923457858551\n",
            "epoch is 1000,loss is 0.7497394609451294 and acc is 0.692, learning_Rate is 0.04762131530072861\n",
            "epoch is 1200,loss is 0.7489730381965637 and acc is 0.677, learning_Rate is 0.04717203641681212\n",
            "epoch is 1400,loss is 0.7339443726539612 and acc is 0.701, learning_Rate is 0.04673115566147951\n",
            "epoch is 1600,loss is 0.7300653667449951 and acc is 0.6966666666666667, learning_Rate is 0.04629843974258068\n",
            "epoch is 1800,loss is 0.7410551919937134 and acc is 0.6833333333333333, learning_Rate is 0.04587366392953806\n",
            "epoch is 2000,loss is 0.7193880953788757 and acc is 0.6876666666666666, learning_Rate is 0.045456611664166556\n",
            "epoch is 2200,loss is 0.7298426699638366 and acc is 0.6933333333333334, learning_Rate is 0.0450470741925312\n",
            "epoch is 2400,loss is 0.7308172106742858 and acc is 0.6893333333333334, learning_Rate is 0.04464485021652753\n",
            "epoch is 2600,loss is 0.7148775210380555 and acc is 0.696, learning_Rate is 0.04424974556396301\n",
            "epoch is 2800,loss is 0.7298134903907776 and acc is 0.6793333333333333, learning_Rate is 0.04386157287600334\n",
            "epoch is 3000,loss is 0.722540225982666 and acc is 0.7056666666666667, learning_Rate is 0.043480151310926564\n",
            "epoch is 3200,loss is 0.7140691685676575 and acc is 0.683, learning_Rate is 0.043105306263201\n",
            "epoch is 3400,loss is 0.7068454480171203 and acc is 0.6926666666666667, learning_Rate is 0.04273686909696996\n",
            "epoch is 3600,loss is 0.6971908197402954 and acc is 0.6993333333333334, learning_Rate is 0.042374676893088686\n",
            "epoch is 3800,loss is 0.6983822803497315 and acc is 0.6903333333333334, learning_Rate is 0.04201857220891634\n",
            "epoch is 4000,loss is 0.6952639122009278 and acc is 0.6853333333333333, learning_Rate is 0.04166840285011875\n",
            "epoch is 4200,loss is 0.7136803312301636 and acc is 0.702, learning_Rate is 0.041324021653787346\n",
            "epoch is 4400,loss is 0.7058298335075378 and acc is 0.693, learning_Rate is 0.040985286282224684\n",
            "epoch is 4600,loss is 0.7364865770339966 and acc is 0.6836666666666666, learning_Rate is 0.04065205902678971\n",
            "epoch is 4800,loss is 0.7049483699798584 and acc is 0.7003333333333334, learning_Rate is 0.04032420662123473\n",
            "epoch is 5000,loss is 0.6906980381011963 and acc is 0.7153333333333334, learning_Rate is 0.04000160006400256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_1LNqJS5rzp"
      },
      "source": [
        "<h2> Binary Logistic Regression </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJyYfaJE5vnG"
      },
      "source": [
        "# sigmoid activation function\r\n",
        "class Activation_Sigmoid:\r\n",
        "  # computing forward propagation i.e sigmoid(z) = output = 1 / 1 + np.exp(-z)\r\n",
        "  def forward(self,inputs):\r\n",
        "\r\n",
        "    self.output = 1 / (1 + np.exp(-inputs))\r\n",
        "\r\n",
        "  # calculating gradients for sigmoid activaion( derivative of sigmoid(z) = sigmoid(z) * (1 - sigmoid(z)))\r\n",
        "  def backward(self,dvalues):\r\n",
        "\r\n",
        "    self.dinputs = dvalues * self.output * (1-self.output)\r\n",
        "\r\n",
        "  \r\n",
        "# binary cross entropy loss\r\n",
        "class BinaryCrossEntropy(Loss):\r\n",
        "\r\n",
        "  \r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "\r\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7) # clipping predicted values inorder to solve problem when model predicts confidence with 1 or 0\r\n",
        "\r\n",
        "    self.result = - (y_true *np.log(y_pred_clipped) + (1-y_true) * np.log(1-y_pred_clipped))\r\n",
        "\r\n",
        "    #self.result_mean = np.mean(self.result,axis=-1)\r\n",
        "\r\n",
        "    return self.result\r\n",
        "\r\n",
        "  # computing gradients for backward propagation.\r\n",
        "  def backward(self,y_pred,y_true):\r\n",
        "\r\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\r\n",
        " \r\n",
        "    # no of outputs per samples\r\n",
        "    outputs = len(y_pred[0])\r\n",
        "\r\n",
        "    # no of samples\r\n",
        "    samples = len(y_pred)\r\n",
        "\r\n",
        "    self.dinputs = - (y_true / y_pred_clipped - (1-y_true) / (1 - y_pred_clipped) ) / outputs\r\n",
        "\r\n",
        "    # normalizing the gradients\r\n",
        "    self.dinputs = self.dinputs / samples\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ynfGEtL-8mw",
        "outputId": "5bc7a3be-fda5-4197-b7b8-3976c4b9bfb0"
      },
      "source": [
        "X, y = spiral_data(samples=100, classes=2)\r\n",
        "y = y.reshape(-1,1)\r\n",
        "\r\n",
        "dense1 = Dense(2,64,weight_regularizer_l1=5e-4,bias_regularizer_l1=5e-4)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,1)\r\n",
        "\r\n",
        "activation2 = Activation_Sigmoid()\r\n",
        "\r\n",
        "binary_loss = BinaryCrossEntropy()\r\n",
        "\r\n",
        "optimizer = ADAM(decay_rate=5e-7)\r\n",
        "\r\n",
        "for epochs in range(10001):\r\n",
        "\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  activation2.forward(dense2.output)\r\n",
        "\r\n",
        "  data_loss = binary_loss.calculate(activation2.output,y)\r\n",
        "\r\n",
        "  regularization_loss = binary_loss.regularizer_loss(dense1)\r\n",
        "\r\n",
        "  total_loss = data_loss + regularization_loss\r\n",
        "\r\n",
        "  predictions = (activation2.output > 0.5) * 1\r\n",
        "\r\n",
        "  accuracy = np.mean(predictions == y)\r\n",
        "\r\n",
        "  if not epochs % 200:\r\n",
        "\r\n",
        "    print(f'epochs: {epochs}' +  f' acc : {accuracy:.3f}' + f'  loss : {total_loss :.3f}')\r\n",
        "  binary_loss.backward(activation2.output,y)\r\n",
        "  activation2.backward(binary_loss.dinputs)\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizer.update_learning_rate()\r\n",
        "  optimizer.update_params(dense1)\r\n",
        "  optimizer.update_params(dense2)\r\n",
        "  optimizer.update_iterations()\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 0 acc : 0.430  loss : 0.694\n",
            "epochs: 200 acc : 0.830  loss : 0.441\n",
            "epochs: 400 acc : 0.925  loss : 0.302\n",
            "epochs: 600 acc : 0.970  loss : 0.219\n",
            "epochs: 800 acc : 0.975  loss : 0.182\n",
            "epochs: 1000 acc : 0.975  loss : 0.163\n",
            "epochs: 1200 acc : 0.980  loss : 0.151\n",
            "epochs: 1400 acc : 0.985  loss : 0.141\n",
            "epochs: 1600 acc : 0.985  loss : 0.134\n",
            "epochs: 1800 acc : 0.985  loss : 0.128\n",
            "epochs: 2000 acc : 0.990  loss : 0.121\n",
            "epochs: 2200 acc : 0.990  loss : 0.115\n",
            "epochs: 2400 acc : 0.990  loss : 0.111\n",
            "epochs: 2600 acc : 0.990  loss : 0.108\n",
            "epochs: 2800 acc : 0.990  loss : 0.104\n",
            "epochs: 3000 acc : 0.990  loss : 0.102\n",
            "epochs: 3200 acc : 0.995  loss : 0.101\n",
            "epochs: 3400 acc : 0.995  loss : 0.097\n",
            "epochs: 3600 acc : 0.995  loss : 0.094\n",
            "epochs: 3800 acc : 0.995  loss : 0.092\n",
            "epochs: 4000 acc : 0.995  loss : 0.090\n",
            "epochs: 4200 acc : 0.995  loss : 0.088\n",
            "epochs: 4400 acc : 0.995  loss : 0.086\n",
            "epochs: 4600 acc : 0.995  loss : 0.084\n",
            "epochs: 4800 acc : 0.995  loss : 0.082\n",
            "epochs: 5000 acc : 0.995  loss : 0.080\n",
            "epochs: 5200 acc : 0.995  loss : 0.079\n",
            "epochs: 5400 acc : 0.995  loss : 0.077\n",
            "epochs: 5600 acc : 0.995  loss : 0.075\n",
            "epochs: 5800 acc : 0.995  loss : 0.074\n",
            "epochs: 6000 acc : 0.995  loss : 0.072\n",
            "epochs: 6200 acc : 0.995  loss : 0.070\n",
            "epochs: 6400 acc : 0.995  loss : 0.069\n",
            "epochs: 6600 acc : 0.995  loss : 0.068\n",
            "epochs: 6800 acc : 0.995  loss : 0.066\n",
            "epochs: 7000 acc : 0.995  loss : 0.066\n",
            "epochs: 7200 acc : 0.995  loss : 0.065\n",
            "epochs: 7400 acc : 0.995  loss : 0.065\n",
            "epochs: 7600 acc : 0.995  loss : 0.064\n",
            "epochs: 7800 acc : 0.995  loss : 0.063\n",
            "epochs: 8000 acc : 0.995  loss : 0.062\n",
            "epochs: 8200 acc : 0.995  loss : 0.062\n",
            "epochs: 8400 acc : 0.995  loss : 0.061\n",
            "epochs: 8600 acc : 0.995  loss : 0.060\n",
            "epochs: 8800 acc : 0.995  loss : 0.059\n",
            "epochs: 9000 acc : 0.995  loss : 0.059\n",
            "epochs: 9200 acc : 0.995  loss : 0.058\n",
            "epochs: 9400 acc : 0.995  loss : 0.057\n",
            "epochs: 9600 acc : 0.995  loss : 0.056\n",
            "epochs: 9800 acc : 0.995  loss : 0.056\n",
            "epochs: 10000 acc : 0.995  loss : 0.055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdmMCe6r_AF1",
        "outputId": "c04ec390-51b2-4822-addd-08716a37aedd"
      },
      "source": [
        "X, y = spiral_data(samples=100, classes=2)\r\n",
        "y = y.reshape(-1,1)\r\n",
        "\r\n",
        "dense1 = Dense(2,64,weight_regularizer_l1=5e-4,bias_regularizer_l1=5e-4)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,1)\r\n",
        "\r\n",
        "activation2 = Activation_Sigmoid()\r\n",
        "\r\n",
        "binary_loss = BinaryCrossEntropy()\r\n",
        "\r\n",
        "optimizer = ADAM(decay_rate=5e-7)\r\n",
        "\r\n",
        "for epochs in range(10001):\r\n",
        "\r\n",
        "  dense1.forward(X)\r\n",
        "\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "\r\n",
        "  activation2.forward(dense2.output)\r\n",
        "\r\n",
        "  data_loss = binary_loss.calculate(activation2.output,y)\r\n",
        "\r\n",
        "  regularization_loss = binary_loss.regularizer_loss(dense1)\r\n",
        "\r\n",
        "  total_loss = data_loss + regularization_loss\r\n",
        "\r\n",
        "  predictions = (activation2.output > 0.5) * 1\r\n",
        "\r\n",
        "  accuracy = np.mean(predictions == y)\r\n",
        "\r\n",
        "  if not epochs % 200:\r\n",
        "\r\n",
        "    print(f'epochs: {epochs}' +  f' acc : {accuracy:.3f}' + f'  loss : {total_loss :.3f}')\r\n",
        "  binary_loss.backward(activation2.output,y)\r\n",
        "  activation2.backward(binary_loss.dinputs)\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizer.update_learning_rate()\r\n",
        "  optimizer.update_params(dense1)\r\n",
        "  optimizer.update_params(dense2)\r\n",
        "  optimizer.update_iterations()\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs: 0 acc : 0.425  loss : 0.694\n",
            "epochs: 200 acc : 0.860  loss : 0.497\n",
            "epochs: 400 acc : 0.945  loss : 0.324\n",
            "epochs: 600 acc : 0.955  loss : 0.268\n",
            "epochs: 800 acc : 0.970  loss : 0.226\n",
            "epochs: 1000 acc : 0.985  loss : 0.201\n",
            "epochs: 1200 acc : 0.985  loss : 0.180\n",
            "epochs: 1400 acc : 0.985  loss : 0.168\n",
            "epochs: 1600 acc : 0.985  loss : 0.158\n",
            "epochs: 1800 acc : 0.990  loss : 0.149\n",
            "epochs: 2000 acc : 0.995  loss : 0.142\n",
            "epochs: 2200 acc : 0.990  loss : 0.137\n",
            "epochs: 2400 acc : 0.995  loss : 0.132\n",
            "epochs: 2600 acc : 0.995  loss : 0.128\n",
            "epochs: 2800 acc : 0.995  loss : 0.126\n",
            "epochs: 3000 acc : 0.995  loss : 0.123\n",
            "epochs: 3200 acc : 0.995  loss : 0.120\n",
            "epochs: 3400 acc : 0.995  loss : 0.117\n",
            "epochs: 3600 acc : 0.995  loss : 0.115\n",
            "epochs: 3800 acc : 0.995  loss : 0.112\n",
            "epochs: 4000 acc : 0.995  loss : 0.109\n",
            "epochs: 4200 acc : 0.995  loss : 0.107\n",
            "epochs: 4400 acc : 0.995  loss : 0.104\n",
            "epochs: 4600 acc : 0.995  loss : 0.102\n",
            "epochs: 4800 acc : 0.995  loss : 0.100\n",
            "epochs: 5000 acc : 0.995  loss : 0.099\n",
            "epochs: 5200 acc : 0.995  loss : 0.096\n",
            "epochs: 5400 acc : 0.995  loss : 0.094\n",
            "epochs: 5600 acc : 0.995  loss : 0.092\n",
            "epochs: 5800 acc : 0.995  loss : 0.090\n",
            "epochs: 6000 acc : 0.995  loss : 0.088\n",
            "epochs: 6200 acc : 0.995  loss : 0.087\n",
            "epochs: 6400 acc : 0.995  loss : 0.085\n",
            "epochs: 6600 acc : 0.995  loss : 0.084\n",
            "epochs: 6800 acc : 0.995  loss : 0.082\n",
            "epochs: 7000 acc : 0.995  loss : 0.083\n",
            "epochs: 7200 acc : 0.995  loss : 0.081\n",
            "epochs: 7400 acc : 0.995  loss : 0.081\n",
            "epochs: 7600 acc : 0.995  loss : 0.080\n",
            "epochs: 7800 acc : 0.995  loss : 0.079\n",
            "epochs: 8000 acc : 0.995  loss : 0.079\n",
            "epochs: 8200 acc : 0.995  loss : 0.079\n",
            "epochs: 8400 acc : 0.995  loss : 0.078\n",
            "epochs: 8600 acc : 0.995  loss : 0.078\n",
            "epochs: 8800 acc : 0.995  loss : 0.077\n",
            "epochs: 9000 acc : 0.995  loss : 0.076\n",
            "epochs: 9200 acc : 0.995  loss : 0.076\n",
            "epochs: 9400 acc : 0.995  loss : 0.075\n",
            "epochs: 9600 acc : 0.995  loss : 0.074\n",
            "epochs: 9800 acc : 0.995  loss : 0.074\n",
            "epochs: 10000 acc : 0.995  loss : 0.073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXDDVIk0CWBn",
        "outputId": "ad892ac9-6a0a-4adc-8e1e-841b3c9a5bc7"
      },
      "source": [
        "X_test, y_test = spiral_data(samples=100, classes=2)\r\n",
        "y_test = y_test.reshape(-1,1)\r\n",
        "\r\n",
        "dense1.forward(X_test)\r\n",
        "activation1.forward(dense1.output)\r\n",
        "dense2.forward(activation1.output)\r\n",
        "activation2.forward(dense2.output)\r\n",
        "\r\n",
        "data_loss = binary_loss.calculate(activation2.output,y_test)\r\n",
        "predictions = (activation2.output > 0.5) * 1\r\n",
        "accuracy = np.mean(predictions == y_test)\r\n",
        "\r\n",
        "print('loss : {} and accuracy : {}'.format(data_loss,accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss : 0.14201974868774414 and accuracy : 0.985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccHv7Gs0gXpZ"
      },
      "source": [
        "<h2>  Regression  Problem </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SAu8J_5CK1x"
      },
      "source": [
        "class Activation_Linear:\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = inputs\r\n",
        "\r\n",
        "  def backward(self,dvalues):\r\n",
        "    self.dinputs = dvalues.copy()\r\n",
        "\r\n",
        "\r\n",
        "class SquaredLoss(Loss):\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "\r\n",
        "    res = (y_true - y_pred) ** 2\r\n",
        "    loss = np.mean(res,axis=-1)\r\n",
        "\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def backward(self,dvalues,y_true):\r\n",
        "\r\n",
        "    samples = len(dvalues)\r\n",
        "\r\n",
        "    outputs = len(dvalues[0])\r\n",
        "\r\n",
        "    self.dinputs = - 2 * (y_true - dvalues) / outputs\r\n",
        "\r\n",
        "    self.dinputs = self.dinputs / samples\r\n",
        "\r\n",
        "\r\n",
        "class MeanAbsoluteErrorLoss(Loss):\r\n",
        "\r\n",
        "  def forward(self,y_pred,y_true):\r\n",
        "\r\n",
        "    res = np.abs(y_true - y_pred)\r\n",
        "\r\n",
        "    loss = np.mean(res,axis=-1)\r\n",
        "\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def backward(self,dvalues,y_true):\r\n",
        "\r\n",
        "    samples = len(dvalues)\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    self.dinputs = np.sign(y_true -  dvalues)\r\n",
        "\r\n",
        "    self.dinputs = self.dinputs / samples\r\n",
        "\r\n",
        "    \r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC8-ywxUk6Gd",
        "outputId": "4fc3510b-410f-470d-93dd-9c260268f50d"
      },
      "source": [
        "# Create dataset\r\n",
        "from nnfs.datasets import sine_data\r\n",
        "X, y = sine_data()\r\n",
        "\r\n",
        "dense1 = Dense(1,64)\r\n",
        "\r\n",
        "activation1 = ActivationRelu()\r\n",
        "\r\n",
        "dense2 = Dense(64,1)\r\n",
        "\r\n",
        "activation2 = Activation_Linear()\r\n",
        "\r\n",
        "mse_loss = SquaredLoss()\r\n",
        "\r\n",
        "optimizer = ADAM()\r\n",
        "\r\n",
        "for epochs in range(10001):\r\n",
        "\r\n",
        "  dense1.forward(X)\r\n",
        "  activation1.forward(dense1.output)\r\n",
        "  dense2.forward(activation1.output)\r\n",
        "  activation2.forward(dense2.output)\r\n",
        "  data_loss = mse_loss.calculate(activation2.output,y)\r\n",
        "\r\n",
        "  if not epochs % 500:\r\n",
        "    print('loss: {}'.format(data_loss))\r\n",
        "\r\n",
        "  mse_loss.backward(activation2.output,y)\r\n",
        "  activation2.backward(mse_loss.dinputs)\r\n",
        "  dense2.backward(activation2.dinputs)\r\n",
        "  activation1.backward(dense2.dinputs)\r\n",
        "  dense1.backward(activation1.drelu)\r\n",
        "\r\n",
        "  optimizer.update_learning_rate()\r\n",
        "  optimizer.update_params(dense1)\r\n",
        "  optimizer.update_params(dense2)\r\n",
        "  optimizer.update_iterations()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 0.4998947356703105\n",
            "loss: 0.043518963039495415\n",
            "loss: 0.011135711619493172\n",
            "loss: 0.005231296784095935\n",
            "loss: 0.0031474640594874436\n",
            "loss: 0.001885600856963659\n",
            "loss: 0.0013150171454415275\n",
            "loss: 0.0009662300997391419\n",
            "loss: 0.0007442493918078125\n",
            "loss: 0.0006310013734950437\n",
            "loss: 0.0004939538206989323\n",
            "loss: 0.00039110334205195345\n",
            "loss: 0.0011681883554212882\n",
            "loss: 0.0006792317626282358\n",
            "loss: 0.00024200001549748697\n",
            "loss: 0.00021126035641300694\n",
            "loss: 0.0001857243091349871\n",
            "loss: 0.00016569646185267468\n",
            "loss: 0.0002647211502359162\n",
            "loss: 0.000134909358659231\n",
            "loss: 0.00012042269308979097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hD5rSli3TJZ"
      },
      "source": [
        "<h2> Model Object </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8chPE9JnHjF"
      },
      "source": [
        "class Model:\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.layers = []\r\n",
        "\r\n",
        "  def add(self,layer):\r\n",
        "\r\n",
        "    self.layers.append(layer)\r\n",
        "\r\n",
        "  def set(self,loss,optimizer):\r\n",
        "\r\n",
        "    self.loss = loss\r\n",
        "    self.optimizer = optimizer\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvF8AH1n589o"
      },
      "source": [
        "class Input_Layer:\r\n",
        "  def forward(self,inputs):\r\n",
        "    self.output = inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ImApo1e3fNu"
      },
      "source": [
        "\r\n",
        "model = Model()\r\n",
        "\r\n",
        "model.add(Dense(1,64))\r\n",
        "model.add(ActivationRelu())\r\n",
        "model.add(Dense(64,64))\r\n",
        "model.add(ActivationRelu())\r\n",
        "model.add(Dense(64,1))\r\n",
        "model.add(Activation_Linear())\r\n",
        "\r\n",
        "model.set(loss = SquaredLoss(),optimizer = ADAM(decay_rate=1e-3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV4zAMnP4R3c",
        "outputId": "57f7dc9f-6cd0-4efe-f213-40a405e246cd"
      },
      "source": [
        "model.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Dense at 0x7f061763f450>,\n",
              " <__main__.ActivationRelu at 0x7f0617cb0050>,\n",
              " <__main__.Dense at 0x7f0617ca8790>,\n",
              " <__main__.ActivationRelu at 0x7f0617c8a990>,\n",
              " <__main__.Dense at 0x7f0617c8a290>,\n",
              " <__main__.Activation_Linear at 0x7f06175db350>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}